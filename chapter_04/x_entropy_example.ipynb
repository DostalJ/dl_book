{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_ch02_01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zkwbsZYtY7_7"
      },
      "source": [
        "## Banknote classification with fcNN without hidden layer compared to fcNN with hidden layer\n",
        "\n",
        "**Goal:** In this notebook you will do your first classification. You will see that fully connected networks without a hidden layer can only learn linar decision boundaries, while fully connected networks with hidden layers are able to learn non-linear decision boundaries.\n",
        "\n",
        "**Usage:** The idea of the notebook is that you try to understand the provided code. Run it, check the output, and play with it by slightly changing the code. \n",
        "\n",
        "**Dataset:** You work with a banknote data set and classification task. We have 5 features of wavelet transformed images of banknotes:\n",
        ">1. variance  (continuous feature) \n",
        ">2. skewness (continuous feature) \n",
        ">3. curtosis (continuous feature) \n",
        ">4. entropy (continuous feature) \n",
        ">5. class (binary indicating if the banknote is real or fake)  \n",
        "\n",
        "Don't bother too much how these features exactely came from.\n",
        "\n",
        "For this analysis we only use 2 features. \n",
        "\n",
        ">x1: skewness of wavelet transformed image  \n",
        ">x2: entropy of wavelet transformed image\n",
        "\n",
        "\n",
        "**The goal is to classify each banknote to either \"real\" (Y=0) or \"fake\" (Y=1).**\n",
        "\n",
        "\n",
        "**Content:**\n",
        "* visualize the data in a simple scatter plot and color the points by the class label\n",
        "* use the Keras library to build a fcNN without hidden layers (logistic regression). Use SGD with the objective to minimize the crossentropy loss. \n",
        "* visualize the learned decision boundary in a 2D plot\n",
        "* use the Keras library to build a fcNN with a single hidden layer. Use SGD with the objective to minimize the crossentropy loss. \n",
        "* visualize the learned decision boundary in a 2D plot\n",
        "* compare the performace and the decision boundaries of the two models\n",
        "* stack more hidden layers to the model and playaround with the epochs\n",
        "\n",
        "\n",
        "\n",
        "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_02/nb_ch02_01.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DlIPKjGu-UEj"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "In the next two cells, we load all the required libraries and functions from keras and numpy. We also download the data with the 5 featues from the provided url."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJSZXo7S-H5n",
        "outputId": "2aed1b67-f9f6-43e5-b6f6-7f399aade22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load required libraries:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense \n",
        "from keras.utils import to_categorical \n",
        "from keras import optimizers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Ofvh7fPYV9i",
        "outputId": "61d3a448-7d91-45ff-ebcc-385b97812b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load data from url\n",
        "from urllib.request import urlopen\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
        "raw_data = urlopen(url)\n",
        "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
        "print(dataset.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1372, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HlDPWop1_zGM"
      },
      "source": [
        "Let's extract the two featues *x1: skewness of wavelet transformed image* and *x2: entropy of wavelet transformed image*. We print the shape and see that we for X  we have 1372 oberservations with two featues and for Y there are 1372 binary labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHixTIf3fBFL",
        "outputId": "97c14d83-5495-4eef-e590-18a79b26de04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Here we use extract the two features and the label of the dataset\n",
        "X=dataset[:,[1,3]]\n",
        "Y=dataset[:,4]\n",
        "Y_c=to_categorical(Y,2)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1372, 2)\n",
            "(1372,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAeJj88a65Ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fce349fe-fe39-444a-8bb7-7010d93c6066"
      },
      "source": [
        "np.unique(Y,return_counts=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([762, 610]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W2upXCjUBweV"
      },
      "source": [
        "Since the banknotes are described by only 2 features, we can easily visualize the positions of real and fake banknotes in the 2D feature space. You can see that the boundary between the two classes is not separable by a straight line. A curved boundary line will do better. But even then we cannot expect a perfect seperation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-BVHMfQjr8LI"
      },
      "source": [
        "### fcNN with one hidden layer \n",
        "\n",
        "We know that the boundary between the two classes is not descriped very good by a line. Therefore a single neuron is not appropriate to model the probability for a fake banknote based on its two features. To get a more flexible model, we introduce an additional layer between input layer and output layer. This is called hidden layer. Here we use a hidden layer with 8 neurons. We also change the ouputnodes form 1 to 2, to get two ouputs for the probability of real and fake banknote. Because we now have 2 outputs, we use the *softmax* activation function in the output layer. The softmax activation ensures that the output can be interpreted as a probability (see book for details)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lgMpaj2hiHNX",
        "colab": {}
      },
      "source": [
        "# Definition of the network\n",
        "model = Sequential()\n",
        "model.add(Dense(20, batch_input_shape=(None, 2),activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "\n",
        "model.add(Dense(20, activation='relu'))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZ09IBXFidZQ",
        "outputId": "05e44e05-146b-473e-a1db-70bf0eee07e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_141 (Dense)            (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 500)               10500     \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 20)                10020     \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 2)                 42        \n",
            "=================================================================\n",
            "Total params: 1,273,122\n",
            "Trainable params: 1,273,122\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tOX7awYjsWJb"
      },
      "source": [
        "#### Add more hidden layers and play around with the training epochs\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "Exercise: Add more hidden layers to the model and play around with the training epochs. What do you observe? Look at the learned decision boundary. How does the loss and the accuracy change?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1-HPTx5ThF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "52f26518-c42f-4dd0-d4e9-99db2b4acc61"
      },
      "source": [
        "### works only for a quite deep model\n",
        "model.evaluate(X,Y_c)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1372/1372 [==============================] - 1s 478us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6892015112037214, 0.5553935860058309]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XSsQ2eS8DVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a43540b-1844-4db0-e48b-4c929c97f257"
      },
      "source": [
        "-np.log(0.5)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Soz6r8cGRAFh"
      },
      "source": [
        "In the next cell, train the network. In other words, we tune the parameters that were initialized randomly with stochastic gradient descent to minimize our loss function (the categorical crossentropy). We set the batchsize to 128 per updatestep and train for 400 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8-w1vdq-R0Wv"
      },
      "source": [
        "Let's look again at the leraning curve, we plot the accuracy and the loss vs the epochs. You can see that after 100 epochs, we predict around 86% of our data correct and have a loss aorund 0.29 (this values can vary from run to run). This is already alot better than the model without a hidden layer."
      ]
    }
  ]
}