{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_ch04_02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tbTYFRhJoaBu"
      },
      "source": [
        "# Calculation of the cross entropy loss (NLL) function for classification tasks\n",
        "\n",
        "\n",
        "**Goal:** In this notebook you will use Keras to set up a CNN for classification of MNIST images and calculate the cross entropy which is achieved before the CNN was trained. You will first calculate the cross entropy loss for a binary classification problem then for a classification problem with ten classes. You will calculate with basic numpy functions the loss that is expected from random guessing and see that an untrained CNN is not better than guessing.\n",
        "\n",
        "**Usage:** Before working through this notebook we recommend to read chapter 4.2. The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
        "\n",
        "**Dataset:** You work with the MNIST dataset. We have 60'000 28x28 pixel greyscale images of digits (0-9).\n",
        "\n",
        "**Content:**\n",
        "* load the original MNIST data \n",
        "* create a subset the of the data to make it binary classification problem\n",
        "* define a CNN in Keras\n",
        "* evaluation of the cross entropy loss function of the untrained CNN using Keras for only two classes\n",
        "* evaluation of the cross entropy loss function of the untrained CNN using Keras for all classes\n",
        "* implement the loss function yourself using the predicted probabilities and numpy\n",
        "\n",
        "\n",
        "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_04/nb_ch04_02.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEIS4WvpsT5t"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "First you load all the required libraries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6S_hQX5oaBw",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# load required libraries:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras import optimizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4h_3TS0CtJJb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Loading and preparing the MNIST data\n",
        "You download the MNIST data, normalize the pixel-values to be between 0 and 1 and create  a subdataset which only contains images with the labels 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4sZ8lqFfoaB2",
        "outputId": "364f4044-a21e-4918-d0aa-2249b59dfa43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train=x_train / 255 #divide by 255 so that they are in range 0 to 1\n",
        "X_train=np.reshape(X_train, (X_train.shape[0],28,28,1))\n",
        "Y_train=tensorflow.keras.utils.to_categorical(y_train,10) # one-hot encoding\n",
        "\n",
        "# define sub data containing only images with 0 or 1\n",
        "idx = (y_train==0)|(y_train==1)\n",
        "\n",
        "X_train_sub = X_train[idx]\n",
        "Y_train_sub = y_train[idx]\n",
        "Y_train_sub=tensorflow.keras.utils.to_categorical(Y_train_sub,2) # one-hot encoding\n",
        "\n",
        "Y_train.shape, X_train.shape, Y_train_sub.shape, X_train_sub.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 28, 28, 1), (12665, 2), (12665, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZaRFUEP8HJkq"
      },
      "source": [
        "## CNN model\n",
        "\n",
        "You use the same CNN model as in chapter 2. First you will use it to evaluate the loss for only two classes (0 and 1)  and then for all ten classes of the mnist digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSfYQ4f1KYVp",
        "colab": {}
      },
      "source": [
        "# here we define hyperparameter of the CNN\n",
        "batch_size = 128\n",
        "nb_classes = 2  # for the sub data we only have 2 classes\n",
        "img_rows, img_cols = 28, 28\n",
        "kernel_size = (3, 3)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "pool_size = (2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xwh0iYrHJk_",
        "outputId": "0c1b6c82-ac2e-43ac-d385-8b147fe71d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(40))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile model and intitialize weights\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0623 11:37:52.366887 140368466982784 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "blxHZguwHJlG",
        "outputId": "c45b744a-00e3-4158-9c26-d3404c7d0f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "source": [
        "# summarize model along with number of model weights\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 8)         584       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 82        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 35,634\n",
            "Trainable params: 35,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfNsOlQdYeOU",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of the untrained model using Keras\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "\n",
        "*Exercise 1: Compute the cross entropy loss with the Keras and compare the result with the value that you would expect when you have a classification problem with two classes. Remember that the network is untrained and the predictions are basically just random guesses.*  \n",
        "\n",
        "You best use the function model.evaluate(), to get the cross entropy loss for the untrained network. The input for this function are the images and corresponding true labels, the function returns the corssentropy loss and the accuracy of the prediction. Note that we use the sub dataset with only two classes (0 and 1). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2vxdCRSyOaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJMiyTTWyQka",
        "colab_type": "text"
      },
      "source": [
        "Scroll down to see the solution.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_aIMKouYMjH",
        "colab_type": "code",
        "outputId": "78f30697-c984-4556-c167-cb70f37b09be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(X_train_sub, Y_train_sub)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12665/12665 [==============================] - 4s 341us/sample - loss: 0.6909 - acc: 0.4677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6908787820633814, 0.4676668]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZvyZHXFw9aM",
        "colab_type": "text"
      },
      "source": [
        "If you have no idea about the training dataset,  you would expect each class with equal probability and your guess for every image would be 1/nr_of_classes. In the case of 2 classes, you predicit every image with a probability around 0.5. The resulting x-entropy, which is the negative log likelihood, is calculated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wm5gZOPsl2R",
        "colab_type": "code",
        "outputId": "42c3a5ae-98f7-4952-d78b-10c67fc3bfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nr_of_classes=2\n",
        "-np.log(1/nr_of_classes)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HkiaEquwdRF",
        "colab_type": "text"
      },
      "source": [
        "#### Return to the book \n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/Page_turn_icon_A.png\" width=\"120\" align=\"left\" />  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcoPox0YIWXj",
        "colab_type": "text"
      },
      "source": [
        "Let's now work with the full Mnist dataset (all ten classes). You can use the same network architecture as before, the only thing you need to change is the number of outputs nodes, which you set to ten, if you want to work with all digit classes from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0q16a2uIBkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_classes = 10\n",
        "\n",
        "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(40))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile model and intitialize weights\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWx9gqJ6IUpZ",
        "colab_type": "code",
        "outputId": "fabccf74-9571-4b71-de7f-b7d494f1bf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "source": [
        "# summarize model along with number of model weights\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 8)         584       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 16)        2320      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                410       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 35,962\n",
            "Trainable params: 35,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-mY30BBI4LJ",
        "colab_type": "text"
      },
      "source": [
        "Here you predict the probabilities for all images in the train data set. You did not train the network yet, therefore the probabilities will be around 10% for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "430DSTDIHJlP",
        "colab": {}
      },
      "source": [
        "# Calculates the probailities for the training data\n",
        "Pred_prob = model.predict_proba(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTyxe7xMJUKC",
        "colab_type": "code",
        "outputId": "c62aa847-9d2d-4a8e-b01a-e45565f94a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "Pred_prob[0:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0980892 , 0.10156239, 0.10729206, 0.09290784, 0.13197447,\n",
              "        0.10113405, 0.08253013, 0.08996263, 0.09509624, 0.09945098],\n",
              "       [0.09687394, 0.09786396, 0.1102694 , 0.10245232, 0.13668509,\n",
              "        0.10452785, 0.07775477, 0.08944198, 0.09143064, 0.09270005],\n",
              "       [0.09365467, 0.10288849, 0.10690026, 0.09287751, 0.13751161,\n",
              "        0.10155033, 0.08650888, 0.0892804 , 0.09370886, 0.09511901],\n",
              "       [0.09355306, 0.10104182, 0.10930493, 0.10792493, 0.11838662,\n",
              "        0.10044384, 0.08611196, 0.09196585, 0.09831635, 0.09295069],\n",
              "       [0.09381933, 0.09867891, 0.11066364, 0.0970021 , 0.13890381,\n",
              "        0.10495119, 0.0880869 , 0.08708301, 0.08927269, 0.09153841]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcqY_UbNYyP2",
        "colab_type": "code",
        "outputId": "1e99abf2-33e9-46f3-ca25-6fa7cfe1a0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Pred_prob.shape, Y_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W46_Euob-ux",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate the loss function using numpy\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "Exercise 2: Use numpy to calculate the value of the negative log-likelihood loss (=x-entropy) that you expect for the untrained CNN, which you have constructed above to discriminate between the 10 classes in MNIST. Use numpy to empirically determine the x-entropy  that results from the probabilities (Pred_prob), that you have recieved from the untrained CNN by predicting for all images the probabilities for each of the ten classes. To determine the x-entropy of the prediction, you can loop over each example and use its true label (Y_train) and the predicted probability for the true class. Do you get the x-entropy value that you have expected?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-nSWXYadTft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv1IEA74dPF6",
        "colab_type": "text"
      },
      "source": [
        "Scroll down to see the solution.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM1EOk9WLkeh",
        "colab_type": "text"
      },
      "source": [
        "In the next cell you calculate the cross entropy loss of each single image, then you sum all individual losses and divide the sum with the nr of training examples. You take the negative of this result to get the negative log likelihood, also known as categorical cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9GkdLKcY5OU",
        "colab_type": "code",
        "outputId": "f25e00f0-2cd0-4596-f870-b167f09f82a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss=np.zeros(len(X_train))\n",
        "Y=np.argmax(Y_train,axis=1)\n",
        "for i in range(0,len(X_train)):\n",
        "  loss[i]=np.log(Pred_prob[i][Y[i]])\n",
        "-np.sum(loss)/len(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.300853642523289"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOvZwJiMZDg",
        "colab_type": "text"
      },
      "source": [
        "If you have no idea about the training dataset, your guess for every image would be 1/nr_of_classes, in the case with 10 classes, you would predicit every image with a probability around 0.1. The corresponding negative log likelihood is calculated below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWml2J8MKqwQ",
        "colab_type": "code",
        "outputId": "5ee65010-b73e-4cf9-db34-67fb077fde6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nr_of_classes=10\n",
        "-np.log(1/nr_of_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3025850929940455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJElA61ZMeZM",
        "colab_type": "text"
      },
      "source": [
        "You get more or less the same result as if you use the model.evaluate function from Keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n60Ql16SLZac",
        "colab_type": "code",
        "outputId": "afa7e31e-eef7-411a-85df-b6fa627ea924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 8s 141us/sample - loss: 2.3009 - acc: 0.1214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.300853636042277, 0.12145]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}