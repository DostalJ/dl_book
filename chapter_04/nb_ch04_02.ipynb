{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nb_ch04_02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tbTYFRhJoaBu"
      },
      "source": [
        "# Calculation of the cross entropy loss (NLL) function for classification\n",
        "\n",
        "\n",
        "**Goal:** In this notebook you will learn how to calculate the cross entropy loss which is also the negative log likelihood, for an untrained neural network. You will first calculate the cross entropy loss for a classification problem for with two classes and then do the same for ten classes. You will see that an untrained CNN is not better than guessing, we will verify this result by calulation the loss in numpy with the predicted probabilities of a random guess.\n",
        "\n",
        "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
        "\n",
        "**Dataset:** You work with the MNIST dataset. We have 60'000 28x28 pixel greyscale images of digits (0-9).\n",
        "\n",
        "**Content:**\n",
        "* load the original MNIST data \n",
        "* create a subset the of the data to make it binary classification problem\n",
        "* define a CNN in Keras\n",
        "* evaluation of the cross entropy loss function of the untrained CNN using Keras for only two classes\n",
        "* evaluation of the cross entropy loss function of the untrained CNN using Keras for all classes\n",
        "* implement the loss function yourself using the predicted probabilities and numpy\n",
        "\n",
        "\n",
        "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_04/nb_ch04_02.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEIS4WvpsT5t"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "In the next two cells, we load all the required libraries. We download the Mnist data, normalize the pixelvalues to be between 0 and 1 and create  a subdataset where we only use the labels 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y6S_hQX5oaBw",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# load required libraries:\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras import optimizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4h_3TS0CtJJb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Loading and preparing the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4sZ8lqFfoaB2",
        "outputId": "23f164e7-c560-4d64-c2a9-ce1af85aaac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train=x_train / 255 #divide by 255 so that they are in range 0 to 1\n",
        "X_train=np.reshape(X_train, (X_train.shape[0],28,28,1))\n",
        "Y_train=tensorflow.keras.utils.to_categorical(y_train,10) # one-hot encoding\n",
        "\n",
        "idx = (y_train==0)|(y_train==1)\n",
        "\n",
        "X_train_sub = X_train[idx]\n",
        "Y_train_sub = y_train[idx]\n",
        "Y_train_sub=tensorflow.keras.utils.to_categorical(Y_train_sub,2) # one-hot encoding\n",
        "\n",
        "Y_train.shape, X_train.shape, Y_train_sub.shape, X_train_sub.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 28, 28, 1), (12665, 2), (12665, 28, 28, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZaRFUEP8HJkq"
      },
      "source": [
        "## CNN model\n",
        "\n",
        "We use the same CNN model as in chapter 2. First we will use it to evaluate the loss for only two classes (0 and 1)  and then for all ten classes of the mnist digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSfYQ4f1KYVp",
        "colab": {}
      },
      "source": [
        "# here we define hyperparameter of the CNN\n",
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "img_rows, img_cols = 28, 28\n",
        "kernel_size = (3, 3)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "pool_size = (2, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xwh0iYrHJk_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "654c0e59-04bf-41e5-a5d5-c5ef63ad3268"
      },
      "source": [
        "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(40))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile model and intitialize weights\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0617 11:29:38.214396 140627765655424 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "blxHZguwHJlG",
        "outputId": "a2692c7e-a770-4be5-f9cf-bd434458c6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "source": [
        "# summarize model along with number of model weights\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 8)         584       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 16)        2320      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 82        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 35,634\n",
            "Trainable params: 35,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfNsOlQdYeOU",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of the untrained model using Keras\n",
        "In the next cell we use the model.evaluate() function, to get the cross entropy loss for the untrained network. The input for this function are the images and corresponding true labels, the function returns the corssentropy loss and the accuracy of the prediction. Note that we use the sub dataset with only two classes (0 and 1). \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "\n",
        "*Exercise 1: Compute the cross entropy loss with the Keras and compare the result with the value that you would expect when you have a classification problem with two classes. Remember that the network is untrained and the predictions are basically just random guesses.*  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2vxdCRSyOaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJMiyTTWyQka",
        "colab_type": "text"
      },
      "source": [
        "Scroll down to see the solution.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_aIMKouYMjH",
        "colab_type": "code",
        "outputId": "012376cd-75f8-4d3f-a448-0f2b949aa13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_train_sub, Y_train_sub)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12665/12665 [==============================] - 3s 201us/sample - loss: 0.6964 - acc: 0.3660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.69635132471187, 0.3659692]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZvyZHXFw9aM",
        "colab_type": "text"
      },
      "source": [
        "If we have no idea about the training dataset,  our guess for every image would be 1/nr_of_classes, in the case with 2 classes, we predicit every image with a probability around 0.5. the negative log likelihood is calculated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wm5gZOPsl2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e0b9378-d422-479d-e024-6da9a5fd3495"
      },
      "source": [
        "nr_of_classes=2\n",
        "-np.log(1/nr_of_classes)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HkiaEquwdRF",
        "colab_type": "text"
      },
      "source": [
        "#### Return to the book \n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/Page_turn_icon_A.png\" width=\"120\" align=\"left\" />  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcoPox0YIWXj",
        "colab_type": "text"
      },
      "source": [
        "Now we want to work with the full Mnist dataset (all ten classes). Let's use the same network architecture as before, the only thing we change is the number of outputs nodes. We will use ten for all digits from 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0q16a2uIBkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(40))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile model and intitialize weights\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWx9gqJ6IUpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "outputId": "1c9dcf2a-7198-47df-dd26-c875a348d9a5"
      },
      "source": [
        "# summarize model along with number of model weights\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 8)         584       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 16)        2320      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40)                31400     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                410       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 35,962\n",
            "Trainable params: 35,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-mY30BBI4LJ",
        "colab_type": "text"
      },
      "source": [
        "Here we predict the probabilities for all trainimages. We did not train the network, therefore the probabilities will be around 10% for each image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "430DSTDIHJlP",
        "colab": {}
      },
      "source": [
        "# Calculates the probailities for the training data\n",
        "Pred_prob = model.predict_proba(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTyxe7xMJUKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "8e9470f4-5029-4d67-f6e2-63435fdd3bb8"
      },
      "source": [
        "Pred_prob[0:5]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09250782, 0.09867508, 0.10000185, 0.10002195, 0.09862246,\n",
              "        0.10734326, 0.09982929, 0.10206727, 0.10361547, 0.09731548],\n",
              "       [0.09321542, 0.10242418, 0.09866772, 0.09818424, 0.10036258,\n",
              "        0.0991603 , 0.10205214, 0.10095597, 0.10177615, 0.10320126],\n",
              "       [0.094369  , 0.09765789, 0.09780802, 0.09717362, 0.10379422,\n",
              "        0.10682811, 0.10278382, 0.10072705, 0.10344021, 0.09541803],\n",
              "       [0.09513659, 0.10402688, 0.09870471, 0.09828294, 0.10176218,\n",
              "        0.10028785, 0.09771107, 0.10047312, 0.10383096, 0.09978367],\n",
              "       [0.09448312, 0.10221019, 0.09744866, 0.10018276, 0.09825058,\n",
              "        0.10830294, 0.10025328, 0.10058228, 0.10219348, 0.09609275]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcqY_UbNYyP2",
        "colab_type": "code",
        "outputId": "30838673-cc2b-4a14-a912-6d5d5ea8b353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Pred_prob.shape, Y_train.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W46_Euob-ux",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate the loss function using numpy\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "Exercise 2: Calculate the loss function for the Mnist example with ten classes using numpy. Use the predicted probabilities (Pred_prob) and loop over each example with the cocorresponding true label (Y_train). Is this the value that you expected?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-nSWXYadTft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv1IEA74dPF6",
        "colab_type": "text"
      },
      "source": [
        "Scroll down to see the solution.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM1EOk9WLkeh",
        "colab_type": "text"
      },
      "source": [
        "In the next cell we calculate the cross entropy loss of each single image, then we sum all individual losses and divide the sum with the nr of training examples. We take the negative of this result the get the negative negative log likelihood, also know as categorical cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9GkdLKcY5OU",
        "colab_type": "code",
        "outputId": "f25e00f0-2cd0-4596-f870-b167f09f82a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss=np.zeros(len(X_train))\n",
        "Y=np.argmax(Y_train,axis=1)\n",
        "for i in range(0,len(X_train)):\n",
        "  loss[i]=np.log(Pred_prob[i][Y[i]])\n",
        "-np.sum(loss)/len(X_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.300853642523289"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOvZwJiMZDg",
        "colab_type": "text"
      },
      "source": [
        "If we have no idea about the training dataset, our guess for every image would be 1/nr_of_classes, in the case with 10 classes, we predicit every image with a probability around 0.1. the negative log likelihood is calculated below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWml2J8MKqwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ee65010-b73e-4cf9-db34-67fb077fde6f"
      },
      "source": [
        "nr_of_classes=10\n",
        "-np.log(1/nr_of_classes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3025850929940455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJElA61ZMeZM",
        "colab_type": "text"
      },
      "source": [
        "We get the same result if we use the model.evaluate function from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n60Ql16SLZac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "afa7e31e-eef7-411a-85df-b6fa627ea924"
      },
      "source": [
        "model.evaluate(X_train, Y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 8s 141us/sample - loss: 2.3009 - acc: 0.1214\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.300853636042277, 0.12145]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}