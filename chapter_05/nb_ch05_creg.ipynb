{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count regression\n",
    "\n",
    "This notebook demonstrates the use of TFP for regression on count data. The dataset is the number of fish caught in a state park. \n",
    "\n",
    "* Poisson Regression\n",
    "* Zero Infated Poisson Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "try: #If running in colab \n",
    "    import google.colab\n",
    "    IN_COLAB = True \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if (not tf.__version__.startswith('2')): #Checking if tf 2.0 is installed\n",
    "    if IN_COLAB: #If running in colab install tf 2.0\n",
    "        !pip install tensorflow==2.0.0-alpha0\n",
    "        !pip install pip install tfp-nightly==0.7.0-dev20190511\n",
    "    print('Please install tensorflow 2.0 to run this notebook')\n",
    "print('Tensorflow version: ',tf.__version__, ' running in colab?: ', IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFP Version 0.7.0-dev20190511\n",
      "TF  Version 2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import seaborn as sns\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "sns.reset_defaults()\n",
    "#sns.set_style('whitegrid')\n",
    "sns.set_context(context='talk',font_scale=0.7)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "print(\"TFP Version\", tfp.__version__)\n",
    "print(\"TF  Version\",tf.__version__)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating some count data\n",
    "\n",
    "This is just to demonstrate how to use TF distributions for a Poisson distribution, with given parameters. Below we learn the paramters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "1.4142135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10          20       30        40       50         55\n",
    "#123456789012345678901234567890123456789012345678901234\n",
    "dist = tfd.poisson.Poisson(rate = 2) #A\n",
    "vals = np.linspace(0,10,11) #B\n",
    "p = dist.prob(vals) #C\n",
    "print(dist.mean().numpy())  #D\n",
    "print(dist.stddev().numpy())   #E\n",
    "plt.xticks(vals)\n",
    "plt.stem(vals, p)\n",
    "plt.xlabel('Number of Events')\n",
    "plt.ylabel('Probability')\n",
    "plt.savefig('ch05_rain.pdf')\n",
    "\n",
    "#A Poisson distribution with parameter rate 2\n",
    "#B some values for the x-axis in figure 5.rain\n",
    "#C the probability for the values\n",
    "#D the mean value yielding 2.0\n",
    "#E the standard deviation yielding sqrt(2.0) = 1.41..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading some count data\n",
    "\n",
    "We use some traditional count data from: https://stats.idre.ucla.edu/r/dae/zip/. The number fish caught during a state park visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative version\n",
    "# dat = np.loadtxt('https://raw.githubusercontent.com/tensorchiefs/dl_book/master/data/nb_data.csv') \n",
    "# X = dat[...,0:3]\n",
    "# y = dat[...,4]\n",
    "# X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# X_tr.shape, y_tr.shape, X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((167, 4), (167,), (83, 4), (83,), (250, 8))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Fish Data Set\n",
    "# See example 2 from https://stats.idre.ucla.edu/r/dae/zip/ \n",
    "#\"nofish\",\"livebait\",\"camper\",\"persons\",\"child\",\"xb\",\"zg\",\"count\"\n",
    "#dat = np.loadtxt('../data/fish.csv', delimiter=',', skiprows=1)\n",
    "dat = np.loadtxt('https://raw.githubusercontent.com/tensorchiefs/dl_book/master/data/fish.csv',delimiter=',', skiprows=1)\n",
    "X = dat[...,1:5] #\"livebait\",\"camper\",\"persons\",\"child\n",
    "y = np.sqrt(dat[...,7])\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "d = X_tr.shape[1]\n",
    "X_tr.shape, y_tr.shape, X_te.shape, y_te.shape,dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.194179359853515,\n",
       " array([ 0.57213991,  0.47855112,  0.81969841, -1.09167139]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "res = model.fit(X_tr, y_tr)\n",
    "y_t = model.predict(X_tr)\n",
    "np.sqrt(np.mean((y_t - y_tr)**2)),np.mean((y_t - y_tr)**2)\n",
    "res.intercept_,res.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.164414002968976"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals, counts = np.unique(y_tr, return_counts=True)\n",
    "plt.stem(vals, counts)\n",
    "plt.xlabel('Count: number of fish caught')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(-1,40)\n",
    "plt.savefig('ch05_school.pdf')\n",
    "np.max(y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Doing linear regression\n",
    "\n",
    "In this section we do a standard linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = tf.keras.Sequential() #A\n",
    "model_lr.add(tf.keras.layers.Dense(1,input_dim=d, activation='linear')) #B \n",
    "model_lr.compile(loss='mean_squared_error',optimizer=tf.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_lr = model_lr.fit(x=X_tr, y=y_tr, validation_data=(X_te, y_te), epochs=1200, verbose=False, batch_size=len(y_tr));\n",
    "#hist_lr = model_lr.fit(x=X_tr, y=np.sqrt(y_tr), validation_data=(X_te, y_te), epochs=600, verbose=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4321416492263477"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_lr.history['loss'])\n",
    "plt.plot(hist_lr.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epochs')\n",
    "np.mean(hist_lr.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.2070897 ],\n",
       "        [ 0.16893148],\n",
       "        [ 0.6659379 ],\n",
       "        [-0.9786964 ]], dtype=float32), array([-0.33521238], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.194179359853515,\n",
       " array([ 0.57213991,  0.47855112,  0.81969841, -1.09167139]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.intercept_,res.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_tr = model_lr.predict(X_tr)\n",
    "y_t = model.predict(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3475ff1518>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_hat_tr, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3476016dd8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_hat_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3475fbcc18>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF9lJREFUeJzt3X+MHHd5x/HP480FHxByMTlCfLFjSCPTlqtxukocjBANTROZCq4mkbBwCxWNi6pWoFapHBEhUTlyWgsEFbSVCWlBcV3aJFxpCLihSYRA+GAPBxxiu5AIcC4BH0WGBAwx56d/3J45r3d2Z3a/8+N7+35JJ+/Nzc48Oz8ez37nme/X3F0AgHgsKzsAAEA2JG4AiAyJGwAiQ+IGgMiQuAEgMiRuAIgMiRsAIkPiBoDIkLgBIDLn5LHQCy+80NesWZPHogFgSZqenv6hu4+mmTeXxL1mzRo1Go08Fg0AS5KZfTftvDSVAEBkSNwAEBkSNwBEhsQNAJEhcQNAZEjcABCZXMoBAWBQTB6Y0a59R/TU8RNaOTKsm69bq4n1Y7muk8QNAD2aPDCjW+49qBMn5yRJM8dP6JZ7D0pSrsmbphIA6NGufUdOJ+0FJ07Oade+I7mul8QNAD166viJTNNDSZW4zWzEzO42s8NmdsjMrs41KgCIwMqR4UzTQ0l7xf0hSZ9z91dIWifpUH4hAUAcbr5urYaHamdMGx6q6ebr1ua63q43J83sfEmvlfR2SXL35yQ9l2tUABCBhRuQVawqeZmkWUn/bGbrJE1Lepe7/zTXyAAgAhPrx3JP1K3SNJWcI+kKSf/o7usl/VTS9taZzGybmTXMrDE7Oxs4TADAgjSJ+0lJT7r7VPP3uzWfyM/g7rvdve7u9dHRVH2BAwB60DVxu/v3JR01s4XW9tdLeizXqAAAidI+OfkXkvaY2bmSnpD0x/mFBADoJFXidvdHJNVzjgUAkAJPTgJAZEjcABAZEjcARIZuXQGgD7dOHtTeqaOac1fNTFuuWqUdE+O5rpPEDQA9unXyoO7a/73Tv8+5n/49z+RNUwkA9Gjv1NFM00PhihulKWPIJyCkOfdM00MhcaMUZQ35BIRUM2ubpGtmua6XphKUoqwhn4CQtly1KtP0ULjiRinKGvIJ+Rq05q+FG5BUlWAgrBwZ1kybJJ33kE/Iz6A2f+2YGM89UbeiqQSlKGvIJ+SH5q/icMWNUpQ15BPyU2Tz16A1ybQicaM0ZQz5hPwU1fw1qE0yi9FUAiCIopq/aJLhihtAIEU1f1GRROIGEFARzV9UJNFUAiAyVCRxxQ0gMlQkkbgBRGjQK5JoKgGAyJC4ASAyJG4AiEyqNm4z+46kZyTNSfqlu9fzDAoAkCzLzcnfcfcf5hYJACAVmkoAIDJpE7dL+m8zmzazbXkGBADoLG1TyWvcfcbMXiLpATM77O5fWDxDM6Fvk6TVq1cHDhMAsCDVFbe7zzT/PSbpU5KubDPPbnevu3t9dHQ0bJQAgNO6Jm4ze4GZnbfwWtLvSXo078AAAO2laSq5SNKnbH64+XMk/au7fy7XqAAAibombnd/QtK6AmIBAKRAOSAARIbEDQCRoVtXAOhDGSPOk7gBoEdljThPUwkA9KisEee54gZyUsZXaBSrrBHnueIGcrDwFXrm+Am5fvUVevLATNmhIaCkkeXzHnGexA3koKyv0ChWWSPO01QC5KCsr9AoVlkjzpO4gRysHBnWTJsknfdXaBSvjBHnaSoBclDWV2gMBq64gRyU9RUag4HEDeSkjK/QGAw0lQBAZEjcABAZEjcARIbEDQCRIXEDQGRI3AAQGRI3AESGxA0AkSFxA0BkSNwAEJnUidvMamZ2wMzuyzMgAEBnWfoqeZekQ5JelFMsAArG8GpxSnXFbWaXSHqDpDvyDQdAURheLV5pm0o+KOmvJZ3KMRYABWJ4tXh1Tdxm9vuSjrn7dJf5tplZw8was7OzwQIEkA+GV4tXmjbujZLeaGabJC2X9CIzu8vdty6eyd13S9otSfV63YNHCuAs/bRR5zG8Gm3mxeh6xe3ut7j7Je6+RtJbJD3YmrQBFK/fNurQw6vRZl4c6riBSPXbRj2xfkw7N49rbGRYJmlsZFg7N4/3fIVMm3lxMg1d5u4PS3o4l0gAZBKijTrk8Gq0mReHK24gUklt0f20UfejavEsZSRuIFKh26iXWjxLGaO8A5FaaOKoShVH1eJZysw9fOVevV73RqMRfLkAsFSZ2bS719PMyxU3APShjNp1EjcA9Gihdn2hDHKhdl1Srsmbm5MA0KOyatdJ3ADQo7Jq10ncANCjsmrXSdwA0KOyate5OQkAPSqrdp3EDQB9CNnfS1o0lQBAZEjcABAZmkoAoA88OQkAEeHJSQCIDE9OAkBkeHISACLDk5MAojd5YEYbb39QL9v+GW28/cElP8I7T04CiFpZN+rKxJOTAKLW6UbdUk3cEk9OAohYWTfqBlHXxG1my83sK2b2dTP7ppm9r4jAAMSlrBt1gyhNU8kvJF3j7s+a2ZCkL5rZZ919f86xAZmU8QRb2ar0mW++bu0ZbdxSMTfqBlHXxO3zw8A/2/x1qPkTfmh4oA+DeGOsap+5rBt1gyjVzUkzq0malvRrkj7i7lO5RgVkNIg3xqr4mcu4UTeIUt2cdPc5d3+VpEskXWlmr2ydx8y2mVnDzBqzs7Oh4wQ6GsQbY4P4mTEvU1WJux+X9JCk69v8bbe71929Pjo6Gio+IJVBvDE2iJ8Z89JUlYya2Ujz9bCkayUdzjswIIuynmArU4jPPGhPOuahjG2Ypo37Ykkfb7ZzL5P07+5+X75hAdkM4o2xfj9z1W5uxqisbWjzRSNh1et1bzQawZcLIJyNtz+omTbt4WMjw/rS9mtKiCg+IbehmU27ez3NvDw5CQwobm72j25dARSKm5v9o1tXAIUaxBu6odGtK4BCTawfU+O7P9LeqaOac1fNTG/+7f4eoKnSI/hFoFtXAIWaPDCje6ZnNNcsUJhz1z3TM6pfuqKnxDOoVSp06wqgMKEHui1r4NxBROIGBlToigiqVIpD4gYGVOiKCKpUikPiBgZU6IoIqlSKw81JYECFrojIo0olSZWqV8qIhcQNDLCQFRGhq1Q6racq1StlxUJTCYAgiqoqqVL1SlmxkLgBBFFUVUmVqlfoqwRA1IqqKqlS9Qp9lQCIWlFVJVWqXqGvEgBRK6rfjioNmkFfJaiMKpVaJck7xhi2AQYXiRtnqFKpVZK8Y4xhG1RRUdutSvuHckBUQpVKrZLkHWMM26CKKAcsLhYSN85QpVKrJHnHGMM2qCLKAbtPD4XEjTNUqdQqSd4xxrANqohywO7TQyFx4wxVKrVKkneMMWyDKqIcsLhYuDmJM1Sp1CpJ3jHGsA2qiHLA4mIxb3YIkziD2SpJn5B0kSSXtNvdP9TpPfV63RuNRrAgAWCpM7Npd6+nmTfNFfcvJf2Vu3/NzM6TNG1mD7j7Y31FiYFCXTQQTtfE7e5PS3q6+foZMzskaUwSiRupVKnuFlgKMt2cNLM1ktZLmsojGCxNVaq7BZaC1InbzF4o6R5J73b3n7T5+zYza5hZY3Z2NmSMiFyV6m6BpSBV4jazIc0n7T3ufm+7edx9t7vX3b0+OjoaMkZErkp1t8BS0DVxm5lJ+pikQ+7+gfxDwlJTpbpbYClIc8W9UdIfSrrGzB5p/mzKOS4sIRPrx7Rz87jGRoZlksZGhrVz8zg3JoEepakq+aIkKyAWLGEhB6UFBh1PTi5Rt04e1N6po5pzV81MW65apR0T46XFU0Qdd7/rCB1ju+VJ1Xjib8FbP/plfenxH53+feNlK7TnpqtLiwfpdH1yshc8OVmuWycP6q793ztr+tYNq0tJ3q113NJ8G3fI5pJ+1xE6xnbLG6qZ5NLJU78650Jvhyxak/YCknc5sjw5SSdTS9DeqaOZpuetiDruftcROsZ2yzs552ck7X7X0a92SbvTdFQHiXsJmkv4FpU0PW9F1HH3u47QMWZ5H/XsyIrEvQTVrP295KTpeSuijrvfdYSOMcv7qGdHViTuJWjLVasyTc9bEXXc/a4jdIztljdUMw0tO/M/zzLr2TdetiLTdFQHiXsJ2jExrq0bVp++wq6ZlXZjUiqmjrvfdYSOsd3ydt2wTrtuXFeZevY9N119VpLmxmQcqCoBgAqgqgQAljASNwBEhicnIxbTqDKhYu30NOJMS1nd5S95gR74y9eFCL8nRTy9GnIdIfZRUcdkTMd+HmjjjlQRTyOGEirWtk8jLjPJ5h9uaaes5F3E06sh1xFiHxV1TMZ07GdBG/cAiGlUmVCxtn0a8ZQnJm1J+taxn2ZaRyhFPL0ach0h9lFRx2RMx35eSNyRimlUmVCxVvGzJSni6dWQ6wixj4o6JmM69vNC4o5UTKPKhIq1ip8tSRFPr4ZcR4h9VNQxGdOxnxcSd6RiGlUmVKxtn0ZcZvO97iW4/CUvyLSOUIp4ejXkOkLso6KOyZiO/bxQVRKphZswMdxZDxVr0nIWplWpqmTh5mCeVSUh1xFiHxV1TMZ07OeFqhIAqACqSgBgCSNxA0BkSNwAEBkSNwBEhsQNAJHpmrjN7E4zO2ZmjxYREACgszR13P8i6cOSPpFvKIMtdG9n/fQal0evdq94z/36+aI+RZbXTIdv29Tz8jr1EtjrNly8zPOHh2QmHf/ZyZ73R7vtWL90RaXqj6vcy16VYytbqjpuM1sj6T53f2WahVLHnU3o3s766TUuj17tWpP2gl6Td9peArNsw3bLXCzr/kjajrVlprlTvcUYWpV72atybHmhjjsyoXs766fXuDx6tWuXtDtN7yZtL4FZtmG7Zfa6LCl5ey1O2r0sN6Qq97JX5diqIFjiNrNtZtYws8bs7GyoxQ6E0L2d9dNrXBG92vUrRI91vcyXZb1ZtldZvdpVuZe9KsdWBcESt7vvdve6u9dHR0dDLXYghO7trJ9e44ro1a5fIXqs62W+LOvNsr3K6tWuyr3sVTm2KqCppAJC93bWT69xefRqtzyh976k6d2k7SUwyzZst8xelyUlb6/ast5jDK3KvexVObYqSFMOuFfSlyWtNbMnzewd+Yc1WCbWj2nn5nGNjQzLJI2NDPd1E2bHxLi2blh9+qqvZpb65mI/701y+LZNZyXpfqpK2m2vXTeu064b1vW8DVuXOTI8pAueP9Tz/kjaju+/sfcYQwt93A1KbFVA74AAUAFUlQDAEkbiBoDIkLgBIDIkbgCIDIkbACJD4gaAyJC4ASAyJG4AiAyJGwAiQ+IGgMiQuAEgMiRuAIgMiRsAIkPiBoDIkLgBIDIkbgCIDIkbACJD4gaAyJC4ASAyJG4AiAyJGwAiQ+IGgMiQuAEgMuekmcnMrpf0IUk1SXe4++2hA5k8MKNd+47oqeMntHJkWDdft1YT68dSv//WyYPaO3VUc+6qmWnLVatUv3SFdu07opnjJ86Yd+NlK7TnpqszxyPprOUtrGvHxHimGBeYScPnLNPPTp7KvMy3fvTL+tLjPzpr+sbLVrSd/p3b39A1Rklas/0zZ00zSd4ybXnNdPi2TT0vszXOLPuldZ+2c9F552rqPdemim/xsp86fkLnDw/JTDr+s5M9HY/S2ftn42UrJCnzZ04bcy9x9vv+Vu3OwzTnBrIx99bTsWUGs5qk/5V0raQnJX1V0hZ3fyzpPfV63RuNRuogJg/M6JZ7D+rEybnT04aHatq5eTzVQXTr5EHdtf97Z02vLTPNnWr/+TqdMO3iGaqZ5NLJhOVt3bC64wGaFGMnnZaZlLS76Za82yXYTtIk7yzLzLpfukmbvLstO8vxKGXbP70m737Pm37f3yrpGO92bmCemU27ez3NvGmaSq6U9G13f8Ldn5P0b5Le1E+ArXbtO3LWCXPi5Jx27TuS6v17p462nZ6UtCV1PKnaxXNyzhOTdqcY0v4963t6Sdp5+Plc5//4s8q6X7r5wTPPpZqv27KzHI9Stv3T677s97zp9/2tko7XXo59dJYmcY9JWrzln2xOO4OZbTOzhpk1ZmdnMwXxVMLX3qTprea6fGvIKu16s8TQS4yhP1fsetkvIZed5/p70e950+/7WyUdrxzH4QW7Oenuu9297u710dHRTO9dOTKcaXqrmlmm9XWTdr1ZYuglxtCfK3a97JeQy85z/b3o97zp9/2tko5XjuPw0iTuGUmrFv1+SXNaMDdft1bDQ7Uzpg0P1U7fEOxmy1Wr2k6vLUs+YBZuFKWNZ6hmGuqwvKQY0v4963s6xV+k5bWwJ2XW/dLNReedm2q+bsvOcjxK2fZPr/uy3/Om3/e3Sjpeezn20VmaxP1VSZeb2cvM7FxJb5H06ZBBTKwf087N4xobGZZJGhsZznSDZMfEuLZuWH36f/aambZuWK3337hOY22uHrrdDGoXz64b1mlXm+UtrKvbzZfWGBeYSc8fOnM3pFnmnpuuTjzhk6anqSpJmqddek5bVZK0zNY4s+yXNLJUlbTu85HhIV3w/KGejkep/f7ZeNmKzJ85S8xZ4+z3/a2SzkNuTIbXtapEksxsk6QPar4c8E53v63T/FmrSgBg0GWpKklVx+3u90u6v6+oAABB8OQkAESGxA0AkSFxA0BkSNwAEBkSNwBEJlU5YOaFms1K+m6X2S6U9MPgK+9fFeMipvSqGFcVY5KqGdcgx3Spu6d67DyXxJ1qxWaNtDWLRapiXMSUXhXjqmJMUjXjIqZ0aCoBgMiQuAEgMmUm7t0lrruTKsZFTOlVMa4qxiRVMy5iSqG0Nm4AQG9oKgGAyBSWuM3sRjP7ppmdMrPEO7Rmdr2ZHTGzb5vZ9gLiWmFmD5jZt5r/XpAw35yZPdL8Cdqt7aJ1dPzsZvY8M/tk8+9TZrYmjzgyxvR2M5tdtG3+pICY7jSzY2b2aMLfzcz+vhnzN8zsigrE9Doz+/Gi7fTeAmJaZWYPmdljzXPvXW3mKWNbpYmr0O1lZsvN7Ctm9vVmTO9rM0/h518idy/kR9KvS1or6WFJ9YR5apIel/RySedK+rqk38g5rr+TtL35erukv02Y79mc4+j62SX9maR/ar5+i6RPViCmt0v6cFHHUXOdr5V0haRHE/6+SdJnNd+N+AZJUxWI6XWS7it4O10s6Yrm6/M0P+h36/4rY1uliavQ7dX8/C9svh6SNCVpQ8s8hZ5/nX4Ku+J290Pu3m0U0twHJm7jTZI+3nz9cUkTOa8vSZrPvjjWuyW93izXcaHK2B9dufsXJHUaYfdNkj7h8/ZLGjGzi0uOqXDu/rS7f635+hlJh3T2eLFlbKs0cRWq+fmfbf461PxpvQFY9PmXqGpt3KkGJg7sInd/uvn6+5IuSphveXMw5P1mlkdyT/PZT8/j7r+U9GNJL84hliwxSdKbm1+z7zazKoxTVcZxlMbVza/inzWz3yxyxc2v9es1fyW5WKnbqkNcUsHby8xqZvaIpGOSHnD3xG1V0PmXKNVACmmZ2eclvbTNn97j7v8Zcl1ZdIpr8S/u7maWVGZzqbvPmNnLJT1oZgfd/fHQsUbovyTtdfdfmNmfav6K5JqSY6qir2n+GHq2OaLUpKTLi1ixmb1Q0j2S3u3uPylinWl0iavw7eXuc5JeZWYjkj5lZq9097b3LMoWNHG7++/2uYhcBibuFJeZ/cDMLnb3p5tfEY8lLGOm+e8TZvaw5q8SQibuNJ99YZ4nzewcSedL+r+AMWSOyd0Xr/8Ozd8zKFvuA1xntTgxufv9ZvYPZnahu+faB4aZDWk+Oe5x93vbzFLKtuoWV1nbq7m+42b2kKTrJS1O3EWff4mq1lSS+8DEbXxa0tuar98m6axvBmZ2gZk9r/n6QkkbJT0WOI40n31xrDdIetCbd0py0jWmlvbQN2q+vbJsn5b0R82KiQ2SfryoOawUZvbShfZQM7tS8+derid9c30fk3TI3T+QMFvh2ypNXEVvLzMbbV5py8yGJV0r6XDLbEWff8mKugsq6Q803372C0k/kLSvOX2lpPsXzbdJ83eZH9d8E0vecb1Y0v9I+pakz0ta0Zxel3RH8/WrJR3UfFXFQUnvyCmWsz67pL+R9Mbm6+WS/kPStyV9RdLLC9g+3WLaKembzW3zkKRXFBDTXklPSzrZPKbeIemdkt7Z/LtJ+kgz5oNKqGIqOKY/X7Sd9kt6dQExvUbzN9i+IemR5s+mCmyrNHEVur0k/ZakA82YHpX03jbHeuHnX9IPT04CQGSq1lQCAOiCxA0AkSFxA0BkSNwAEBkSNwBEhsQNAJEhcQNAZEjcABCZ/wdAWx4dhGf3MgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_t, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9167475680236996, 1.157702617038657)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_te = model_lr.predict(X_te)\n",
    "np.sqrt(np.mean((y_hat_te - y_te)**2)),np.mean(np.abs(y_hat_te - y_te)) #(8.540011535073502, 6.001939873018208)\n",
    "#np.sqrt(np.mean((y_hat_te**2 - y_te)**2)),np.mean(np.abs(y_hat_te**2 - y_te)) #(8.540011535073502, 6.001939873018208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted ')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_te, y_hat_te**2)\n",
    "plt.xlabel('True ')\n",
    "plt.ylabel('Predicted ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the likelihood in LR (reestimating $\\sigma$)\n",
    "\n",
    "In linear regression, we assuming that the $\\sigma$ is constant. To calculate the absolute NLL, we need to estimate this quantity from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5682615784009, 1.169159524380048)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_tr = model_lr.predict(X_tr) #Trainingset\n",
    "sigma_hat_2 = np.var(y_hat_tr.flatten() - y_tr)\n",
    "NLL_n =  0.5*np.log(2 * np.pi * sigma_hat_2) + 0.5*np.mean((y_hat_te - y_te)**2)/sigma_hat_2\n",
    "NLL_n,sigma_hat_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood LR also estimating the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = tf.Variable(5.66, dtype='float32')\n",
    "\n",
    "def gauss(out): \n",
    "    return tfd.Normal(loc=out, scale=sig)\n",
    " \n",
    "inputs = tf.keras.layers.Input(shape=(X_tr.shape[1],))  \n",
    "mu = tf.keras.layers.Dense(1)(inputs)\n",
    "#out = tf.concat([mu, sig],axis=-1)\n",
    "p_y = tfp.layers.DistributionLambda(gauss)(mu) \n",
    "model_lr = Model(inputs=inputs, outputs=p_y) \n",
    "\n",
    "\n",
    "def negloglik(y_true, y_hat): \n",
    "  return -tf.reduce_mean(y_hat.log_prob(y_true))\n",
    "\n",
    "model_lr.compile(tf.optimizers.Adam(learning_rate=0.01),loss=negloglik) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[ 0.5288104 ],\n",
       "        [ 0.28163075],\n",
       "        [-1.0573573 ],\n",
       "        [-0.3436523 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.trainable_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.8838384, shape=(), dtype=float32)   <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.6568375>\n",
      "tf.Tensor(2.807916, shape=(), dtype=float32)   <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.5482597>\n",
      "tf.Tensor(2.7459824, shape=(), dtype=float32)   <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.4469414>\n",
      "tf.Tensor(2.6921997, shape=(), dtype=float32)   <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.3459353>\n",
      "tf.Tensor(2.6475334, shape=(), dtype=float32)   <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.2451916>\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.optimizers.RMSprop()\n",
    "loss_values = np.zeros((500))\n",
    "for e in range(500):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model_lr(X_tr)\n",
    "        loss_value = -tf.reduce_mean(y_hat.log_prob(y_tr))\n",
    "        loss_values[e] = loss_value\n",
    "        weights =  model_lr.trainable_weights\n",
    "        weights.append(sig)\n",
    "        grads = tape.gradient(loss_value, weights)              \n",
    "        #weights =  model_lr.trainable_weights\n",
    "        optimizer.apply_gradients(zip(grads,weights))\n",
    "        if e % 100 == 0:\n",
    "            print(loss_value, \" \", sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Variable 'dense_2/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       "  array([[ 1.0205046 ],\n",
       "         [ 0.7652056 ],\n",
       "         [-0.5603665 ],\n",
       "         [ 0.15271682]], dtype=float32)>,\n",
       "  <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.49467352], dtype=float32)>],\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.145687>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.trainable_weights, sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167 samples, validate on 83 samples\n",
      "Epoch 1/2000\n",
      "167/167 [==============================] - 0s 491us/sample - loss: 2.6166 - val_loss: 2.6269\n",
      "Epoch 2/2000\n",
      "167/167 [==============================] - 0s 89us/sample - loss: 2.6121 - val_loss: 2.6264\n",
      "Epoch 3/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.6098 - val_loss: 2.6255\n",
      "Epoch 4/2000\n",
      "167/167 [==============================] - 0s 83us/sample - loss: 2.6080 - val_loss: 2.6239\n",
      "Epoch 5/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.6064 - val_loss: 2.6224\n",
      "Epoch 6/2000\n",
      "167/167 [==============================] - 0s 112us/sample - loss: 2.6046 - val_loss: 2.6208\n",
      "Epoch 7/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.6032 - val_loss: 2.6193\n",
      "Epoch 8/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.6017 - val_loss: 2.6180\n",
      "Epoch 9/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.6005 - val_loss: 2.6168\n",
      "Epoch 10/2000\n",
      "167/167 [==============================] - 0s 98us/sample - loss: 2.5992 - val_loss: 2.6157\n",
      "Epoch 11/2000\n",
      "167/167 [==============================] - 0s 109us/sample - loss: 2.5978 - val_loss: 2.6148\n",
      "Epoch 12/2000\n",
      "167/167 [==============================] - 0s 97us/sample - loss: 2.5966 - val_loss: 2.6141\n",
      "Epoch 13/2000\n",
      "167/167 [==============================] - 0s 134us/sample - loss: 2.5956 - val_loss: 2.6132\n",
      "Epoch 14/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5944 - val_loss: 2.6121\n",
      "Epoch 15/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5934 - val_loss: 2.6110\n",
      "Epoch 16/2000\n",
      "167/167 [==============================] - 0s 115us/sample - loss: 2.5925 - val_loss: 2.6101\n",
      "Epoch 17/2000\n",
      "167/167 [==============================] - 0s 112us/sample - loss: 2.5916 - val_loss: 2.6094\n",
      "Epoch 18/2000\n",
      "167/167 [==============================] - 0s 120us/sample - loss: 2.5910 - val_loss: 2.6087\n",
      "Epoch 19/2000\n",
      "167/167 [==============================] - 0s 144us/sample - loss: 2.5901 - val_loss: 2.6081\n",
      "Epoch 20/2000\n",
      "167/167 [==============================] - 0s 162us/sample - loss: 2.5895 - val_loss: 2.6075\n",
      "Epoch 21/2000\n",
      "167/167 [==============================] - 0s 114us/sample - loss: 2.5888 - val_loss: 2.6069\n",
      "Epoch 22/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5881 - val_loss: 2.6064\n",
      "Epoch 23/2000\n",
      "167/167 [==============================] - 0s 101us/sample - loss: 2.5876 - val_loss: 2.6059\n",
      "Epoch 24/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5870 - val_loss: 2.6054\n",
      "Epoch 25/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5865 - val_loss: 2.6049\n",
      "Epoch 26/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5861 - val_loss: 2.6044\n",
      "Epoch 27/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5856 - val_loss: 2.6040\n",
      "Epoch 28/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5853 - val_loss: 2.6037\n",
      "Epoch 29/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5848 - val_loss: 2.6034\n",
      "Epoch 30/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5844 - val_loss: 2.6031\n",
      "Epoch 31/2000\n",
      "167/167 [==============================] - 0s 99us/sample - loss: 2.5840 - val_loss: 2.6027\n",
      "Epoch 32/2000\n",
      "167/167 [==============================] - 0s 110us/sample - loss: 2.5838 - val_loss: 2.6023\n",
      "Epoch 33/2000\n",
      "167/167 [==============================] - 0s 121us/sample - loss: 2.5835 - val_loss: 2.6020\n",
      "Epoch 34/2000\n",
      "167/167 [==============================] - 0s 92us/sample - loss: 2.5831 - val_loss: 2.6015\n",
      "Epoch 35/2000\n",
      "167/167 [==============================] - 0s 103us/sample - loss: 2.5828 - val_loss: 2.6012\n",
      "Epoch 36/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5825 - val_loss: 2.6010\n",
      "Epoch 37/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5822 - val_loss: 2.6009\n",
      "Epoch 38/2000\n",
      "167/167 [==============================] - 0s 88us/sample - loss: 2.5821 - val_loss: 2.6008\n",
      "Epoch 39/2000\n",
      "167/167 [==============================] - 0s 119us/sample - loss: 2.5820 - val_loss: 2.6005\n",
      "Epoch 40/2000\n",
      "167/167 [==============================] - 0s 109us/sample - loss: 2.5817 - val_loss: 2.6003\n",
      "Epoch 41/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5814 - val_loss: 2.6001\n",
      "Epoch 42/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5814 - val_loss: 2.5999\n",
      "Epoch 43/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5813 - val_loss: 2.5997\n",
      "Epoch 44/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5811 - val_loss: 2.5994\n",
      "Epoch 45/2000\n",
      "167/167 [==============================] - 0s 94us/sample - loss: 2.5810 - val_loss: 2.5993\n",
      "Epoch 46/2000\n",
      "167/167 [==============================] - 0s 97us/sample - loss: 2.5808 - val_loss: 2.5992\n",
      "Epoch 47/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5807 - val_loss: 2.5992\n",
      "Epoch 48/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5806 - val_loss: 2.5991\n",
      "Epoch 49/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5805 - val_loss: 2.5989\n",
      "Epoch 50/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5803 - val_loss: 2.5986\n",
      "Epoch 51/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5802 - val_loss: 2.5984\n",
      "Epoch 52/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5801 - val_loss: 2.5984\n",
      "Epoch 53/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5800 - val_loss: 2.5983\n",
      "Epoch 54/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5800 - val_loss: 2.5983\n",
      "Epoch 55/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5799 - val_loss: 2.5983\n",
      "Epoch 56/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5798 - val_loss: 2.5981\n",
      "Epoch 57/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5798 - val_loss: 2.5981\n",
      "Epoch 58/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5797 - val_loss: 2.5980\n",
      "Epoch 59/2000\n",
      "167/167 [==============================] - 0s 110us/sample - loss: 2.5796 - val_loss: 2.5981\n",
      "Epoch 60/2000\n",
      "167/167 [==============================] - 0s 106us/sample - loss: 2.5796 - val_loss: 2.5979\n",
      "Epoch 61/2000\n",
      "167/167 [==============================] - 0s 96us/sample - loss: 2.5796 - val_loss: 2.5978\n",
      "Epoch 62/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5795 - val_loss: 2.5976\n",
      "Epoch 63/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5794 - val_loss: 2.5975\n",
      "Epoch 64/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5793 - val_loss: 2.5974\n",
      "Epoch 65/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5793 - val_loss: 2.5975\n",
      "Epoch 66/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5794 - val_loss: 2.5976\n",
      "Epoch 67/2000\n",
      "167/167 [==============================] - 0s 90us/sample - loss: 2.5793 - val_loss: 2.5972\n",
      "Epoch 68/2000\n",
      "167/167 [==============================] - 0s 121us/sample - loss: 2.5792 - val_loss: 2.5971\n",
      "Epoch 69/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5792 - val_loss: 2.5970\n",
      "Epoch 70/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5792 - val_loss: 2.5969\n",
      "Epoch 71/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.588 - 0s 102us/sample - loss: 2.5792 - val_loss: 2.5968\n",
      "Epoch 72/2000\n",
      "167/167 [==============================] - 0s 95us/sample - loss: 2.5790 - val_loss: 2.5970\n",
      "Epoch 73/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5792 - val_loss: 2.5976\n",
      "Epoch 74/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5792 - val_loss: 2.5974\n",
      "Epoch 75/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5790 - val_loss: 2.5968\n",
      "Epoch 76/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5791 - val_loss: 2.5966\n",
      "Epoch 77/2000\n",
      "167/167 [==============================] - 0s 95us/sample - loss: 2.5790 - val_loss: 2.5966\n",
      "Epoch 78/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5790 - val_loss: 2.5966\n",
      "Epoch 79/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5789 - val_loss: 2.5967\n",
      "Epoch 80/2000\n",
      "167/167 [==============================] - 0s 101us/sample - loss: 2.5788 - val_loss: 2.5967\n",
      "Epoch 81/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5789 - val_loss: 2.5966\n",
      "Epoch 82/2000\n",
      "167/167 [==============================] - 0s 94us/sample - loss: 2.5788 - val_loss: 2.5966\n",
      "Epoch 83/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.570 - 0s 63us/sample - loss: 2.5788 - val_loss: 2.5966\n",
      "Epoch 84/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5787 - val_loss: 2.5963\n",
      "Epoch 85/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5787 - val_loss: 2.5962\n",
      "Epoch 86/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5789 - val_loss: 2.5959\n",
      "Epoch 87/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5788 - val_loss: 2.5959\n",
      "Epoch 88/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5788 - val_loss: 2.5959\n",
      "Epoch 89/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5787 - val_loss: 2.5958\n",
      "Epoch 90/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5787 - val_loss: 2.5959\n",
      "Epoch 91/2000\n",
      "167/167 [==============================] - 0s 48us/sample - loss: 2.5786 - val_loss: 2.5959\n",
      "Epoch 92/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5786 - val_loss: 2.5960\n",
      "Epoch 93/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5786 - val_loss: 2.5959\n",
      "Epoch 94/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5786 - val_loss: 2.5957\n",
      "Epoch 95/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5786 - val_loss: 2.5958\n",
      "Epoch 96/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5785 - val_loss: 2.5959\n",
      "Epoch 97/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5787 - val_loss: 2.5964\n",
      "Epoch 98/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5786 - val_loss: 2.5964\n",
      "Epoch 99/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5786 - val_loss: 2.5959\n",
      "Epoch 100/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5785 - val_loss: 2.5958\n",
      "Epoch 101/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5785 - val_loss: 2.5960\n",
      "Epoch 102/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5784 - val_loss: 2.5962\n",
      "Epoch 103/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5785 - val_loss: 2.5965\n",
      "Epoch 104/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5786 - val_loss: 2.5961\n",
      "Epoch 105/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5785 - val_loss: 2.5960\n",
      "Epoch 106/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 107/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5785 - val_loss: 2.5956\n",
      "Epoch 108/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5788 - val_loss: 2.5959\n",
      "Epoch 109/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 110/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 111/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 112/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5784 - val_loss: 2.5958\n",
      "Epoch 113/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5784 - val_loss: 2.5960\n",
      "Epoch 114/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5784 - val_loss: 2.5960\n",
      "Epoch 115/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5784 - val_loss: 2.5957\n",
      "Epoch 116/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5783 - val_loss: 2.5955\n",
      "Epoch 117/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5784 - val_loss: 2.5954\n",
      "Epoch 118/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5783 - val_loss: 2.5956\n",
      "Epoch 119/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5785 - val_loss: 2.5961\n",
      "Epoch 120/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5784 - val_loss: 2.5959\n",
      "Epoch 121/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5784 - val_loss: 2.5958\n",
      "Epoch 122/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5784 - val_loss: 2.5959\n",
      "Epoch 123/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5784 - val_loss: 2.5958\n",
      "Epoch 124/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5783 - val_loss: 2.5957\n",
      "Epoch 125/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5784 - val_loss: 2.5954\n",
      "Epoch 126/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 127/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5785 - val_loss: 2.5951\n",
      "Epoch 128/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5784 - val_loss: 2.5951\n",
      "Epoch 129/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 130/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5783 - val_loss: 2.5953\n",
      "Epoch 131/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 132/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 133/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 134/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 135/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 136/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 137/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5783 - val_loss: 2.5950\n",
      "Epoch 138/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5950\n",
      "Epoch 139/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 140/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 141/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 142/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 143/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 144/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.580 - 0s 55us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 145/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 146/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 147/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5960\n",
      "Epoch 148/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5785 - val_loss: 2.5963\n",
      "Epoch 149/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5786 - val_loss: 2.5958\n",
      "Epoch 150/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 151/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 152/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 153/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 154/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 155/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 156/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 157/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 158/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 159/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 160/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 161/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 162/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 163/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 164/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 165/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 166/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 167/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 168/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 169/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 170/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 171/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 172/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 173/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 174/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 175/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 176/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 177/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 178/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 179/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 180/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 181/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 182/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 183/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5956\n",
      "Epoch 184/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 185/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 186/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 187/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 188/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 189/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 190/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 191/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 192/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 193/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 194/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 195/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 196/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 197/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 198/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 199/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 200/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 201/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 202/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 203/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 204/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 205/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 206/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 207/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 208/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 209/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 210/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 211/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 212/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5783 - val_loss: 2.5946\n",
      "Epoch 213/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 214/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5783 - val_loss: 2.5946\n",
      "Epoch 215/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 216/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 217/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 218/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 219/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 220/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 221/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 222/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 223/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 224/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 225/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 226/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 227/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 228/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 229/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 230/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 231/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 232/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 233/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 234/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 235/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 236/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 237/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 238/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 239/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 240/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 241/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 242/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 243/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 244/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 245/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 246/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5951\n",
      "Epoch 247/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 248/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 249/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 250/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 251/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 252/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 253/2000\n",
      "167/167 [==============================] - 0s 83us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 254/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 255/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 256/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 257/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 258/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 259/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.574 - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 260/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 261/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 262/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 263/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 264/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 265/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 266/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 267/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 268/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 269/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 270/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 271/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5955\n",
      "Epoch 272/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 273/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 274/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 275/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5783 - val_loss: 2.5947\n",
      "Epoch 276/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 277/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 278/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 279/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 280/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 281/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 282/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 283/2000\n",
      "167/167 [==============================] - 0s 46us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 284/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 285/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 286/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 287/2000\n",
      "167/167 [==============================] - 0s 48us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 288/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 289/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 290/2000\n",
      "167/167 [==============================] - 0s 45us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 291/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 292/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 293/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 294/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 295/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 296/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 297/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 298/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 299/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 300/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 301/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 302/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 303/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 304/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 305/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 306/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 307/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 308/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 309/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 310/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 311/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 312/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5783 - val_loss: 2.5953\n",
      "Epoch 313/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 314/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 315/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 316/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 317/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 318/2000\n",
      "167/167 [==============================] - 0s 48us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 319/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 320/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 321/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5784 - val_loss: 2.5944\n",
      "Epoch 322/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 323/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 324/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.576 - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 325/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 326/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 327/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 328/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 329/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 330/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 331/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 332/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 333/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 334/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 335/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 336/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 337/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 338/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 339/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 340/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 341/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 342/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 343/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 344/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 345/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 346/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 347/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 348/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 349/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 350/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 351/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 352/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 353/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 354/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 355/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 356/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 357/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 358/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 359/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 360/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 361/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 362/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 363/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 364/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 365/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5784 - val_loss: 2.5941\n",
      "Epoch 366/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5785 - val_loss: 2.5941\n",
      "Epoch 367/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 368/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 369/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 370/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 371/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 372/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 373/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 374/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 375/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.577 - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 376/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 377/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 378/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 379/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 380/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 381/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 382/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 383/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 384/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 385/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 386/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 387/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 388/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 389/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 390/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 391/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 392/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 393/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 394/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 395/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 396/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 397/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 398/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 399/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 400/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 401/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 402/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 403/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 404/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 405/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 406/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 407/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 408/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 409/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 410/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 411/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 412/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 413/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.569 - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 414/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 415/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 416/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 417/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 418/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 419/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 420/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 421/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 422/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5784 - val_loss: 2.5944\n",
      "Epoch 423/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 424/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 425/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 426/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 427/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 428/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 429/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 430/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 431/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 432/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5784 - val_loss: 2.5944\n",
      "Epoch 433/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 434/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 435/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 436/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 437/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 438/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5943\n",
      "Epoch 439/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 440/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 441/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 442/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 443/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 444/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 445/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 446/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 447/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 448/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 449/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 450/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 451/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 452/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 453/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 454/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 455/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 456/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 457/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 458/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 459/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 460/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 461/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 462/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 463/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 464/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 465/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 466/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 467/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 468/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 469/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 470/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 471/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 472/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 473/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 474/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 475/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 476/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 477/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 478/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 479/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 480/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 481/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 482/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 483/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 484/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 485/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 486/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 487/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 488/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 489/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 490/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 491/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 492/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 493/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 494/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 495/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 496/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 497/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 498/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 499/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 500/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 501/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 502/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 503/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 504/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 505/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 506/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 507/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 508/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 509/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 510/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 511/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 512/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 513/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 514/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 515/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 516/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 517/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 518/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 519/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 520/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.570 - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 521/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 522/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 523/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 524/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 525/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 526/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 527/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 528/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 529/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 530/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 531/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 532/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 533/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 534/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 535/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 536/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 537/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 538/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 539/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 540/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 541/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 542/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 543/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 544/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 545/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 546/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 547/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 548/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 549/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 550/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 551/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5951\n",
      "Epoch 552/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 553/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 554/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 555/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 556/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 557/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 558/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 559/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 560/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 561/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 562/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 563/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 564/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 565/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 566/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 567/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 568/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 569/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 570/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 571/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 572/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.573 - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 573/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 574/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 575/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 576/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 577/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 578/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 579/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 580/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 581/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 582/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 583/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 584/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 585/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 586/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 587/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 588/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 589/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 590/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 591/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 592/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 593/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 594/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 595/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 596/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 597/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 598/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 599/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 600/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 601/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 602/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 603/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 604/2000\n",
      "167/167 [==============================] - -0s -123us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 605/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 606/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 607/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 608/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 609/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 610/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 611/2000\n",
      "167/167 [==============================] - 0s 48us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 612/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 613/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 614/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 615/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 616/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5954\n",
      "Epoch 617/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5958\n",
      "Epoch 618/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 619/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 620/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 621/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 622/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 623/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 624/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 625/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 626/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 627/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 628/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 629/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 630/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 631/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 632/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 633/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.571 - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 634/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 635/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5785 - val_loss: 2.5945\n",
      "Epoch 636/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5784 - val_loss: 2.5946\n",
      "Epoch 637/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.581 - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 638/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 639/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 640/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 641/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 642/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 643/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 644/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 645/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 646/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 647/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 648/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 649/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 650/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 651/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 652/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 653/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 654/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 655/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 656/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 657/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 658/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 659/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 660/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5787 - val_loss: 2.5943\n",
      "Epoch 661/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 662/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5779 - val_loss: 2.5950\n",
      "Epoch 663/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 664/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 665/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 666/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 667/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 668/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 669/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 670/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 671/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 672/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.584 - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 673/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 674/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 675/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 676/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 677/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 678/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 679/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 680/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 681/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 682/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 683/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 684/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 685/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 686/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 687/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 688/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 689/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.572 - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 690/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 691/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 692/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 693/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 694/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 695/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 696/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 697/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 698/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 699/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 700/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 701/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 702/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 703/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 704/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 705/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 706/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 707/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 708/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 709/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 710/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 711/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 712/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 713/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 714/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 715/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 716/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 717/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 718/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 719/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 720/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 721/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 722/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 723/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 724/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 725/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 726/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 727/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 728/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 729/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 730/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 731/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 732/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 733/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 734/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 735/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 736/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 737/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 738/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 739/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 740/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 741/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 742/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 743/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 744/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 745/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 746/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 747/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 748/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 749/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 750/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 751/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.585 - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 752/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 753/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 754/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 755/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 756/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 757/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 758/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 759/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 760/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 761/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 762/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 763/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 764/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 765/2000\n",
      "167/167 [==============================] - 0s 92us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 766/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 767/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 768/2000\n",
      "167/167 [==============================] - 0s 46us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 769/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5783 - val_loss: 2.5956\n",
      "Epoch 770/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5784 - val_loss: 2.5955\n",
      "Epoch 771/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 772/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 773/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5784 - val_loss: 2.5945\n",
      "Epoch 774/2000\n",
      "167/167 [==============================] - 0s 93us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 775/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5951\n",
      "Epoch 776/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 777/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 778/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 779/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 780/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 781/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 782/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 783/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 784/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 785/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 786/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 787/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 788/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 789/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 790/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 791/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 792/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 793/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 794/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 795/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 796/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 797/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 798/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 799/2000\n",
      "167/167 [==============================] - 0s 86us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 800/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 801/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 802/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 803/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 804/2000\n",
      "167/167 [==============================] - 0s 89us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 805/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 806/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 807/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 808/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 809/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 810/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 811/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 812/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 813/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 814/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 815/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 816/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 817/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 818/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 819/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 820/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 821/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 822/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 823/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 824/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 825/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 826/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 827/2000\n",
      "167/167 [==============================] - 0s 83us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 828/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 829/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 830/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 831/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 832/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 833/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 834/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 835/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 836/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 837/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 838/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 839/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 840/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 841/2000\n",
      "167/167 [==============================] - 0s 90us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 842/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 843/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5780 - val_loss: 2.5942\n",
      "Epoch 844/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 845/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5783 - val_loss: 2.5942\n",
      "Epoch 846/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 847/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 848/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 849/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 850/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 851/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 852/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 853/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 854/2000\n",
      "167/167 [==============================] - 0s 83us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 855/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 856/2000\n",
      "167/167 [==============================] - 0s 86us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 857/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 858/2000\n",
      "167/167 [==============================] - 0s 83us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 859/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 860/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 861/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 862/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 863/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 864/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 865/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 866/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 867/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 868/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 869/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 870/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 871/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 872/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 873/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 874/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 875/2000\n",
      "167/167 [==============================] - 0s 90us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 876/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 877/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 878/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 879/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 880/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 881/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 882/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 883/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 884/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 885/2000\n",
      "167/167 [==============================] - 0s 86us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 886/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 887/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 888/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 889/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 890/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 891/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 892/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 893/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 894/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 895/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 896/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 897/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 898/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 899/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 900/2000\n",
      "167/167 [==============================] - 0s 105us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 901/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 902/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 903/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 904/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 905/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 906/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 907/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 908/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 909/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 910/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 911/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 912/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 913/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 914/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 915/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 916/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 917/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.590 - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 918/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 919/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 920/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 921/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 922/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 923/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 924/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 925/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 926/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 927/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 928/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 929/2000\n",
      "167/167 [==============================] - 0s 88us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 930/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 931/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 932/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 933/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 934/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 935/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 936/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 937/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 938/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 939/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 940/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 941/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 942/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 943/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 944/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 945/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 946/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 947/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 948/2000\n",
      "167/167 [==============================] - 0s 95us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 949/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 950/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 951/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 952/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 953/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 954/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 955/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 956/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 957/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 958/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 959/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 960/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 961/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 962/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 963/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 964/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 965/2000\n",
      "167/167 [==============================] - 0s 82us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 966/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 967/2000\n",
      "167/167 [==============================] - 0s 89us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 968/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 969/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 970/2000\n",
      "167/167 [==============================] - 0s 91us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 971/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 972/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 973/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 974/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 975/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 976/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5943\n",
      "Epoch 977/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 978/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 979/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 980/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 981/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 982/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 983/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 984/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 985/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 986/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 987/2000\n",
      "167/167 [==============================] - 0s 87us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 988/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 989/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 990/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 991/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 992/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 993/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 994/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 995/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 996/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 997/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 998/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 999/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1000/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1001/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1002/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1003/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1004/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1005/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1006/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1007/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1008/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1009/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1010/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.575 - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1011/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1012/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1013/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1014/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5783 - val_loss: 2.5941\n",
      "Epoch 1015/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5784 - val_loss: 2.5941\n",
      "Epoch 1016/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1017/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1018/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1019/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1020/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1021/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1022/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1023/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1024/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1025/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1026/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1027/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1028/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1029/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1030/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1031/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1032/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1033/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1034/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1035/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1036/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1037/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1038/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1039/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1040/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1041/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1042/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1043/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1044/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1045/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1046/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1047/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1048/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1049/2000\n",
      "167/167 [==============================] - 0s 89us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1050/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1051/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1052/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1053/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5782 - val_loss: 2.5957\n",
      "Epoch 1054/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5785 - val_loss: 2.5958\n",
      "Epoch 1055/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 1056/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1057/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1058/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1059/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1060/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1061/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1062/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1063/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1064/2000\n",
      "167/167 [==============================] - 0s 46us/sample - loss: 2.5781 - val_loss: 2.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1065/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1066/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1067/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1068/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1069/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1070/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1071/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1072/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1073/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1074/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1075/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1076/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1077/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1078/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1079/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1080/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1081/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1082/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1083/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1084/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1085/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1086/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1087/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1088/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1089/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1090/2000\n",
      "167/167 [==============================] - 0s 81us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1091/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1092/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1093/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1094/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1095/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1096/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1097/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1098/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1099/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1100/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1101/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1102/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1103/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1104/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1105/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1106/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1107/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1108/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1109/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1110/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1111/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1112/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1113/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1114/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.570 - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1115/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1116/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1117/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1118/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1119/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1120/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1121/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1122/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1123/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1124/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1125/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1126/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1127/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1128/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1129/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1130/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1131/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1132/2000\n",
      "167/167 [==============================] - 0s 112us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1133/2000\n",
      "167/167 [==============================] - 0s 147us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1134/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1135/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1136/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1137/2000\n",
      "167/167 [==============================] - 0s 48us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1138/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1139/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1140/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1141/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1142/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5784 - val_loss: 2.5945\n",
      "Epoch 1143/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1144/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1145/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 1146/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1147/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1148/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1149/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1150/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1151/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1152/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1153/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1154/2000\n",
      "167/167 [==============================] - 0s 77us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1155/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1156/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5952\n",
      "Epoch 1157/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 1158/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1159/2000\n",
      "167/167 [==============================] - 0s 88us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1160/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1161/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5783 - val_loss: 2.5945\n",
      "Epoch 1162/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1163/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5779 - val_loss: 2.5955\n",
      "Epoch 1164/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5785 - val_loss: 2.5958\n",
      "Epoch 1165/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1166/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1167/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1168/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1169/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1170/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1171/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1172/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1173/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1174/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1175/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1176/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1177/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1178/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1179/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1180/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1181/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1182/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1183/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1184/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1185/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1186/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5952\n",
      "Epoch 1187/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1188/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1189/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1190/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1191/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1192/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1193/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1194/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5783 - val_loss: 2.5956\n",
      "Epoch 1195/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 1196/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1197/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1198/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1199/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1200/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1201/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1202/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1203/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1204/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1205/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1206/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1207/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1208/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1209/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1210/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1211/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1212/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1213/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1214/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1215/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1216/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5785 - val_loss: 2.5954\n",
      "Epoch 1217/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1218/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1219/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1220/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1221/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1222/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1223/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1224/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1225/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 1226/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1227/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1228/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1229/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1230/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1231/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1232/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1233/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1234/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1235/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1236/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1237/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1238/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1239/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1240/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1241/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1242/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1243/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1244/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1245/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1246/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1247/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1248/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1249/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1250/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1251/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1252/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1253/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1254/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1255/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1256/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1257/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1258/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1259/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1260/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5783 - val_loss: 2.5953\n",
      "Epoch 1261/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1262/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1263/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1264/2000\n",
      "167/167 [==============================] - 0s 78us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1265/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5783 - val_loss: 2.5948\n",
      "Epoch 1266/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1267/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1268/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1269/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1270/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1271/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1272/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1273/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1274/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1275/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1276/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1277/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1278/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1279/2000\n",
      "167/167 [==============================] - 0s 131us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1280/2000\n",
      "167/167 [==============================] - 0s 84us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1281/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1282/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1283/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1284/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1285/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1286/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1287/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1288/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1289/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1290/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1291/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1292/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1293/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1294/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1295/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1296/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1297/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1298/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1299/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1300/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1301/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1302/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5955\n",
      "Epoch 1303/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5956\n",
      "Epoch 1304/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5783 - val_loss: 2.5951\n",
      "Epoch 1305/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1306/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1307/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1308/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1309/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1310/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1311/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1312/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1313/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1314/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1315/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1316/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1317/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1318/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.574 - 0s 55us/sample - loss: 2.5785 - val_loss: 2.5944\n",
      "Epoch 1319/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1320/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5780 - val_loss: 2.5951\n",
      "Epoch 1321/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5955\n",
      "Epoch 1322/2000\n",
      "167/167 [==============================] - 0s 85us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1323/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1324/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1325/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1326/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1327/2000\n",
      "167/167 [==============================] - 0s 79us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1328/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1329/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1330/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1331/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1332/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1333/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1334/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1335/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1336/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1337/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1338/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1339/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1340/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1341/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 1342/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1343/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1344/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1345/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1346/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1347/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1348/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1349/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1350/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1351/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1352/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1353/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1354/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1355/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1356/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1357/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1358/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1359/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1360/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1361/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1362/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1363/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1364/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1365/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1366/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1367/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1368/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1369/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1370/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1371/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1372/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1373/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1374/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1375/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1376/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1377/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1378/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1379/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1380/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1381/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1382/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1383/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1384/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1385/2000\n",
      "167/167 [==============================] - 0s 47us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1386/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1387/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1388/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1389/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1390/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1391/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1392/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1393/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1394/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1395/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1396/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1397/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1398/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1399/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 1400/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5941\n",
      "Epoch 1401/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5784 - val_loss: 2.5942\n",
      "Epoch 1402/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1403/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1404/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1405/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1406/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1407/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1408/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1409/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1410/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1411/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1412/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1413/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1414/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1415/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1416/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1417/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1418/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1419/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1420/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1421/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1422/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1423/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1424/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1425/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1426/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1427/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1428/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1429/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1430/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1431/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1432/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1433/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1434/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1435/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1436/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1437/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1438/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1439/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1440/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1441/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1442/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1443/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1444/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1445/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1446/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1447/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1448/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1449/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1450/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1451/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1452/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1453/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1454/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1455/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1456/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1457/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1458/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1459/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1460/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1461/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1462/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1463/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1464/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1465/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1466/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1467/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1468/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1469/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1470/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1471/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1472/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1473/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5953\n",
      "Epoch 1474/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5783 - val_loss: 2.5955\n",
      "Epoch 1475/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 1476/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1477/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1478/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1479/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1480/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1481/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 1482/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 1483/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5783 - val_loss: 2.5942\n",
      "Epoch 1484/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1485/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1486/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1487/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5952\n",
      "Epoch 1488/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5784 - val_loss: 2.5945\n",
      "Epoch 1489/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1490/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1491/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1492/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1493/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1494/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1495/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1496/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5946\n",
      "Epoch 1497/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1498/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1499/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1500/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1501/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1502/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.575 - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1503/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1504/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1505/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1506/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1507/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1508/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1509/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1510/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1511/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1512/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1513/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1514/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1515/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1516/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1517/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1518/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1519/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1520/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1521/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5954\n",
      "Epoch 1522/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1523/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1524/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1525/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1526/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1527/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1528/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1529/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1530/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1531/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1532/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1533/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1534/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1535/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1536/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1537/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1538/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1539/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1540/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5955\n",
      "Epoch 1541/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1542/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1543/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1544/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1545/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1546/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1547/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1548/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1549/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1550/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1551/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1552/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1553/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1554/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1555/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1556/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5779 - val_loss: 2.5951\n",
      "Epoch 1557/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5784 - val_loss: 2.5955\n",
      "Epoch 1558/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5783 - val_loss: 2.5947\n",
      "Epoch 1559/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1560/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1561/2000\n",
      "167/167 [==============================] - 0s 73us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1562/2000\n",
      "167/167 [==============================] - 0s 75us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1563/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1564/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1565/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1566/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1567/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1568/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1569/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1570/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1571/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1572/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1573/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1574/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1575/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1576/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5783 - val_loss: 2.5942\n",
      "Epoch 1577/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1578/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1579/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1580/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1581/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1582/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1583/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1584/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1585/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1586/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1587/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1588/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1589/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1590/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1591/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1592/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1593/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1594/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1595/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1596/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5783 - val_loss: 2.5958\n",
      "Epoch 1597/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5784 - val_loss: 2.5952\n",
      "Epoch 1598/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1599/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1600/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1601/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1602/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1603/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1604/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1605/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1606/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1607/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1608/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1609/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1610/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1611/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1612/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1613/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1614/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1615/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1616/2000\n",
      "167/167 [==============================] - 0s 49us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1617/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1618/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1619/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1620/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1621/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1622/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5955\n",
      "Epoch 1623/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1624/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1625/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1626/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1627/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1628/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1629/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1630/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1631/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1632/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5783 - val_loss: 2.5947\n",
      "Epoch 1633/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5783 - val_loss: 2.5947\n",
      "Epoch 1634/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1635/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1636/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1637/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1638/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1639/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1640/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1641/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1642/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1643/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1644/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1645/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5952\n",
      "Epoch 1646/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 1647/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1648/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1649/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1650/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1651/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1652/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1653/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1654/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1655/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1656/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1657/2000\n",
      "167/167 [==============================] - 0s 50us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1658/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1659/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1660/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1661/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1662/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1663/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1664/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1665/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1666/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1667/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1668/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1669/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1670/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1671/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1672/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1673/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1674/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1675/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1676/2000\n",
      "167/167 [==============================] - 0s 76us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1677/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1678/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1679/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1680/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1681/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1682/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1683/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1684/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1685/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1686/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1687/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1688/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1689/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1690/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1691/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1692/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1693/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1694/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1695/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1696/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1697/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1698/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1699/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1700/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1701/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1702/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1703/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1704/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1705/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1706/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1707/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1708/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1709/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1710/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1711/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1712/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1713/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1714/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1715/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1716/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1717/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1718/2000\n",
      "167/167 [==============================] - 0s 80us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1719/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1720/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1721/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5782 - val_loss: 2.5952\n",
      "Epoch 1722/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1723/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1724/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1725/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1726/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1727/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1728/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1729/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1730/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1731/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1732/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1733/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1734/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1735/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1736/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1737/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1738/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1739/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1740/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1741/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1742/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1743/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1744/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1745/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1746/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1747/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1748/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1749/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1750/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1751/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1752/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1753/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1754/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.583 - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1755/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1756/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1757/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1758/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1759/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1760/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5786 - val_loss: 2.5956\n",
      "Epoch 1761/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1762/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1763/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1764/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1765/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1766/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1767/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1768/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1769/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1770/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1771/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1772/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1773/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1774/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1775/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1776/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1777/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1778/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1779/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1780/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1781/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1782/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1783/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1784/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5783 - val_loss: 2.5946\n",
      "Epoch 1785/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1786/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1787/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1788/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1789/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1790/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1791/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1792/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1793/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1794/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1795/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1796/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1797/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1798/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1799/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1800/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1801/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1802/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1803/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1804/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1805/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1806/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1807/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1808/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1809/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1810/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1811/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1812/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1813/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1814/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1815/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1816/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5783 - val_loss: 2.5943\n",
      "Epoch 1817/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1818/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1819/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 1820/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1821/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1822/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1823/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5943\n",
      "Epoch 1824/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1825/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1826/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1827/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1828/2000\n",
      "167/167 [==============================] - 0s 72us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1829/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1830/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1831/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1832/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1833/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1834/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1835/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1836/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5943\n",
      "Epoch 1837/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1838/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1839/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1840/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1841/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5941\n",
      "Epoch 1842/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5941\n",
      "Epoch 1843/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5783 - val_loss: 2.5942\n",
      "Epoch 1844/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1845/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1846/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1847/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1848/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1849/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1850/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1851/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1852/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1853/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1854/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1855/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1856/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1857/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1858/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1859/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1860/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5947\n",
      "Epoch 1861/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1862/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1863/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1864/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1865/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1866/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1867/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1868/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1869/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1870/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1871/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.580 - 0s 52us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1872/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5783 - val_loss: 2.5944\n",
      "Epoch 1873/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1874/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1875/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1876/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1877/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1878/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1879/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1880/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1881/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1882/2000\n",
      "167/167 [==============================] - 0s 70us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1883/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1884/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1885/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1886/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1887/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1888/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1889/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5783 - val_loss: 2.5946\n",
      "Epoch 1890/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5783 - val_loss: 2.5946\n",
      "Epoch 1891/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.567 - 0s 51us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1892/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 1893/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5953\n",
      "Epoch 1894/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5951\n",
      "Epoch 1895/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1896/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1897/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1898/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1899/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1900/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1901/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5780 - val_loss: 2.5949\n",
      "Epoch 1902/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1903/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1904/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1905/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1906/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1907/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1908/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1909/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1910/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1911/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1912/2000\n",
      "167/167 [==============================] - 0s 51us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1913/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5942\n",
      "Epoch 1914/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1915/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1916/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5782 - val_loss: 2.5944\n",
      "Epoch 1917/2000\n",
      "167/167 [==============================] - 0s 74us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1918/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5955\n",
      "Epoch 1919/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5782 - val_loss: 2.5949\n",
      "Epoch 1920/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5780 - val_loss: 2.5945\n",
      "Epoch 1921/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1922/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1923/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1924/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1925/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1926/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1927/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1928/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1929/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1930/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1931/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5783 - val_loss: 2.5950\n",
      "Epoch 1932/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5954\n",
      "Epoch 1933/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5783 - val_loss: 2.5954\n",
      "Epoch 1934/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1935/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1936/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1937/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1938/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1939/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1940/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1941/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5953\n",
      "Epoch 1942/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1943/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1944/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5952\n",
      "Epoch 1945/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1946/2000\n",
      "167/167 [==============================] - 0s 53us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1947/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1948/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1949/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1950/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5780 - val_loss: 2.5948\n",
      "Epoch 1951/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1952/2000\n",
      "167/167 [==============================] - 0s 52us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1953/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5951\n",
      "Epoch 1954/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.576 - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1955/2000\n",
      "167/167 [==============================] - 0s 57us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1956/2000\n",
      "167/167 [==============================] - 0s 67us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1957/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1958/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1959/2000\n",
      "167/167 [==============================] - 0s 64us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1960/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1961/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1962/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1963/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5780 - val_loss: 2.5950\n",
      "Epoch 1964/2000\n",
      "167/167 [==============================] - 0s 59us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1965/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5783 - val_loss: 2.5955\n",
      "Epoch 1966/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5784 - val_loss: 2.5956\n",
      "Epoch 1967/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5782 - val_loss: 2.5950\n",
      "Epoch 1968/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1969/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1970/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1971/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1972/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5781 - val_loss: 2.5949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1973/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1974/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1975/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5780 - val_loss: 2.5946\n",
      "Epoch 1976/2000\n",
      "167/167 [==============================] - 0s 71us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1977/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1978/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5944\n",
      "Epoch 1979/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1980/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1981/2000\n",
      "167/167 [==============================] - ETA: 0s - loss: 2.591 - 0s 56us/sample - loss: 2.5782 - val_loss: 2.5945\n",
      "Epoch 1982/2000\n",
      "167/167 [==============================] - 0s 69us/sample - loss: 2.5781 - val_loss: 2.5946\n",
      "Epoch 1983/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5948\n",
      "Epoch 1984/2000\n",
      "167/167 [==============================] - 0s 55us/sample - loss: 2.5781 - val_loss: 2.5950\n",
      "Epoch 1985/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1986/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 1987/2000\n",
      "167/167 [==============================] - 0s 54us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1988/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1989/2000\n",
      "167/167 [==============================] - 0s 68us/sample - loss: 2.5782 - val_loss: 2.5943\n",
      "Epoch 1990/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1991/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5945\n",
      "Epoch 1992/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5780 - val_loss: 2.5944\n",
      "Epoch 1993/2000\n",
      "167/167 [==============================] - 0s 60us/sample - loss: 2.5781 - val_loss: 2.5943\n",
      "Epoch 1994/2000\n",
      "167/167 [==============================] - 0s 62us/sample - loss: 2.5781 - val_loss: 2.5941\n",
      "Epoch 1995/2000\n",
      "167/167 [==============================] - 0s 61us/sample - loss: 2.5782 - val_loss: 2.5942\n",
      "Epoch 1996/2000\n",
      "167/167 [==============================] - 0s 58us/sample - loss: 2.5780 - val_loss: 2.5947\n",
      "Epoch 1997/2000\n",
      "167/167 [==============================] - 0s 65us/sample - loss: 2.5781 - val_loss: 2.5949\n",
      "Epoch 1998/2000\n",
      "167/167 [==============================] - 0s 63us/sample - loss: 2.5782 - val_loss: 2.5948\n",
      "Epoch 1999/2000\n",
      "167/167 [==============================] - 0s 56us/sample - loss: 2.5781 - val_loss: 2.5947\n",
      "Epoch 2000/2000\n",
      "167/167 [==============================] - 0s 66us/sample - loss: 2.5781 - val_loss: 2.5946\n"
     ]
    }
   ],
   "source": [
    "hist_lr = model_lr.fit(x=X_tr, y=y_tr, validation_data=(X_te, y_te), epochs=2000, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Doing Poisson Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the Model\n",
    "\n",
    "$$\n",
    "    Y \\thicksim \\tt{Pois}(exp(w^{T} \\cdot x + b))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10          20       30        40       50         55\n",
    "#123456789012345678901234567890123456789012345678901234\n",
    "inputs = tf.keras.layers.Input(shape=(X_tr.shape[1],))  \n",
    "rate = tf.keras.layers.Dense(1, \n",
    "         activation=tf.exp, #A\n",
    "         bias_initializer='zeros', \n",
    "         kernel_initializer='zeros')(inputs)\n",
    "p_y = tfp.layers.DistributionLambda(tfd.Poisson)(rate) #B \n",
    "\n",
    "model_p = Model(inputs=inputs, outputs=p_y) #C\n",
    "\n",
    "\n",
    "def negloglik(y_true, y_hat): #D\n",
    "  return -y_hat.log_prob(y_true)\n",
    "\n",
    "model_p.compile(tf.optimizers.Adam(learning_rate=0.01), \n",
    "                loss=negloglik)\n",
    "\n",
    "#A Definition of a single layer with one output\n",
    "#B We use exponential of the output to model the rate\n",
    "#C Glueing input and output together. Note that output is a tf.distribution\n",
    "#D Since the second argument is the output of the model it is a distribution. It's as simple as calling log_prob to calculate the NLL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_p = model_p.fit(x=X_tr, y=y_tr, validation_data=(X_te, y_te), epochs=2000, verbose=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_p.history['loss'])\n",
    "plt.plot(hist_p.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "np.mean(hist_p.history['loss'])\n",
    "plt.ylim(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0019479, 1.1606661)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pm = tf.keras.models.Model(inputs=inputs, outputs=p_y.mean()) \n",
    "y_hat_te = model_pm(X_te)\n",
    "np.sqrt(np.mean((y_hat_te - y_te)**2)),np.mean(np.abs(y_hat_te - y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 0.41567588 31\n",
      "Validation Set 0.23069248 15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-0.1,5)\n",
    "plt.ylim(-0.1,5)\n",
    "y_hat_tr = model_pm(X_tr)\n",
    "plt.scatter(y_hat_te, np.random.normal(loc=y_te,scale=0.2), alpha=0.7)\n",
    "plt.xlabel('Counts: predicted by Poisson')\n",
    "plt.ylabel('Counts: true')\n",
    "plt.savefig('ch05_pois.pdf')\n",
    "print('Training Set', np.mean(np.square(y_hat_tr[y_tr == 0])),np.sum(y_hat_tr[y_tr == 0]>0.5))\n",
    "print('Validation Set', np.mean(np.square(y_hat_te[y_te == 0])),np.sum(y_hat_te[y_te == 0]>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Using zero inflated Poisson Regression\n",
    "\n",
    "We define a network with two outputs. One for the poisson mean, one for the prob that there is a zero. \n",
    "\n",
    "First, we define a function which produces the zero inflated poisson distribution. \n",
    "\n",
    "### Definition of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10          20       30        40       50         55\n",
    "#123456789012345678901234567890123456789012345678901234\n",
    "# Listing 5.l_zi\n",
    "def zero_inf(out): \n",
    "    rate = tf.math.exp(out[:,0]) #A \n",
    "    #rate = tf.math.softplus(out[:,0]) #A \n",
    "    s = tf.math.sigmoid(out[:,1:]) #B  \n",
    "    probs = tf.concat([s, 1-s], axis=1) #C \n",
    "    print(np.min(rate), \" \", np.max(rate), \" \", np.min(s), np.max(s),\"  \",np.min(out[:,1:]), np.max(out[:,1:]))\n",
    "    return tfd.Mixture(\n",
    "          cat=tfd.Categorical(probs=probs),#D\n",
    "          components=[\n",
    "            tfd.Deterministic(loc=tf.zeros_like(rate)), #E\n",
    "            tfd.Poisson(rate=rate), #F \n",
    "        ])\n",
    "\n",
    "#A The first component codes for the rate. We use exponential to guaranty values >0.\n",
    "#B The second component codes for the zero inflation, using sigmoid squeezes the value between 0 and 1.\n",
    "#C The two probabilities for zeros or Poissonian  \n",
    "#D The tfd.Categorical allows to create a mixture of two components. \n",
    "#E Zero as a deterministic value \n",
    "#F Value drawn from a Poissonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 14:48:07.878618 139865597429504 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/distributions/mixture.py:154: Categorical.event_size (from tensorflow_probability.python.distributions.categorical) is deprecated and will be removed after 2019-05-19.\n",
      "Instructions for updating:\n",
      "The `event_size` property is deprecated.  Use `num_categories` instead.  They have the same value, but `event_size` is misnamed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.715565   2.7182817   0.7310586 0.9999546    1.0 10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=310897, shape=(5,), dtype=float32, numpy=\n",
       "array([7.3105848e-01, 7.3105848e-01, 7.3105848e-01, 7.3105848e-01,\n",
       "       1.2333754e-04], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testinging the distribution\n",
    "t = np.ones((5,2), dtype=np.float32)\n",
    "t[4,0]=0.999\n",
    "t[4,1]=10\n",
    "zero_inf(t).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"distribution_lambda_2/Exp:0\", shape=(None,), dtype=float32)   Tensor(\"distribution_lambda_2/Exp:0\", shape=(None,), dtype=float32)   Tensor(\"distribution_lambda_2/Sigmoid:0\", shape=(None, 1), dtype=float32) Tensor(\"distribution_lambda_2/Sigmoid:0\", shape=(None, 1), dtype=float32)    Tensor(\"distribution_lambda_2/strided_slice_2:0\", shape=(None, 1), dtype=float32) Tensor(\"distribution_lambda_2/strided_slice_3:0\", shape=(None, 1), dtype=float32)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "distribution_lambda_2 (Distr ((None,), (None,))        0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Definition of the model\n",
    "# 10          20       30        40       50         55\n",
    "#123456789012345678901234567890123456789012345678901234\n",
    "## Definition of the custom parametrized distribution\n",
    "from tensorflow.keras.regularizers import l2\n",
    "inputs = tf.keras.layers.Input(shape=(X_tr.shape[1],))  \n",
    "out = Dense(2,  #A\n",
    "            kernel_regularizer=l2(0.01), #B\n",
    "            bias_regularizer=l2(0.01))(inputs)#B\n",
    "p_y_zi = tfp.layers.DistributionLambda(zero_inf)(out)\n",
    "model_zi = Model(inputs=inputs, outputs=p_y_zi)\n",
    "\n",
    "#A A dense layer is used without activation. The transformation is done in zero_inf listing 5.l_zi \n",
    "#B Using a regularisation prevents the network from learning too large weights. Without regularisation, you run into numerical problems.\n",
    "model_zi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the model (by hand) [optional]\n",
    "\n",
    "The following code trains the NN using a evaluation loop by hand. This help us to find instabilities, which we got rid off using regularisation. See also: https://www.tensorflow.org/beta/guide/keras/training_and_evaluation#part_ii_writing_your_own_training_evaluation_loops_from_scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025004756   0.6247484   0.27135473 0.7500094    -0.98776007 1.0986626\n",
      "0.08532272   1.1142265   0.09467837 0.61448586    -2.2578049 0.46620792\n",
      "0.14538766   1.6905261   0.054261714 0.5380224    -2.8581471 0.15238377\n",
      "0.21434294   2.2604363   0.046183378 0.497875    -3.0278523 -0.00850004\n",
      "0.28375468   2.740368   0.051534653 0.4827182    -2.9125905 -0.06915477\n",
      "0.33627015   3.1020303   0.064576805 0.48248586    -2.673143 -0.07008533\n",
      "0.33518046   3.3501472   0.08308768 0.48957476    -2.4011154 -0.041707024\n",
      "0.3303233   3.4979155   0.10060602 0.49921462    -2.190509 -0.0031415373\n",
      "0.3250746   3.5653749   0.11567133 0.5084693    -2.034076 0.033880368\n",
      "0.3208267   3.5736833   0.13027358 0.5158348    -1.8985419 0.06336038\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.optimizers.RMSprop(learning_rate=0.05)\n",
    "loss_values = np.zeros((1000))\n",
    "for e in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model_zi(X_tr)\n",
    "        loss_value = -tf.reduce_mean(y_hat.log_prob(y_tr))\n",
    "        loss_values[e] = loss_value\n",
    "        grads = tape.gradient(loss_value, model_zi.trainable_weights)\n",
    "        weights =  model_zi.trainable_weights       \n",
    "        optimizer.apply_gradients(zip(grads,weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using keras' build in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negloglik2(y_true, y_hat):\n",
    "    # return -tf.reduce_mean(y_hat.log_prob(y_true))\n",
    "    return -tf.reduce_mean(y_hat.log_prob(tf.reshape(y_true,(-1,))))\n",
    "\n",
    "model_zi.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik2)\n",
    "hist_zi = model_zi.fit(x=X_tr, y=y_tr, validation_data=(X_te, y_te), epochs=2000, verbose=False)#, callbacks=[tb]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0407443450678542, 1.0949766019844864)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGfRJREFUeJzt3Xu4VXW97/H3VyTwATRScrtFQk5mykXUBVKB2mWDtzBvR7fuENMM25llnczd2QqenqfSx/RguzhuTdGtZqfEoykGlgTVUQM2XrEgDyVEchEvhJbg9/wxB8PFYl1YsMZcXN6v55nPGvM3f3OM7xxzrfmZ4/ZbkZlIkgSwW2cXIEnafhgKkqSSoSBJKhkKkqSSoSBJKhkKkqRSpaEQEUsi4qmIWBARc5t5PCJickQsjognI+KIKuuRJLVu9zos48OZuaqFx44HDipuRwHfK35KkjpBZ+8+Ohm4LWseBd4ZEft1ck2StMuqekshgRkRkcD/yswbmzy+P/BCo/tLi7bljTtFxIXAhQA9evQ48v3vf391FUvSTmjevHmrMrNPW/2qDoWRmbksIt4NzIyI5zJzdntnUoTJjQANDQ05d+5mhyckSa2IiD9sSb9Kdx9l5rLi5wpgGjC8SZdlwAGN7vct2iRJnaCyUIiIHhHRa+M0MBp4ukm3+4BxxVlII4BXMnM5kqROUeXuo32BaRGxcTl3ZuZDETEBIDOnAA8CJwCLgXXAeRXWI0lqQ2WhkJnPA4c10z6l0XQC/1xVDZKq9+abb7J06VLeeOONzi5FQPfu3enbty9du3bdqufX4zoFSTuxpUuX0qtXL/r370+xZ0CdJDNZvXo1S5cu5cADD9yqeXT2dQqSdnBvvPEGe++9t4GwHYgI9t57723aajMUJG0zA2H7sa3vhaEgSSoZCpJ2eF26dGHo0KEMGjSIM844g3Xr1rXa/4Mf/GCdKtvxGAqSdnh77LEHCxYs4Omnn+Yd73gHU6ZMabX/r3/96zpVtuMxFCTtVEaNGsXixYsB+Pa3v82gQYMYNGgQ119/fdmnZ8+eACxfvpyjjz663MqYM2cOGzZsYPz48QwaNIjBgwdz3XXXAbBgwQJGjBjBkCFDOOWUU1izZg0Axx57LJdddhnDhw/nfe97H3PmzKnzK+5YnpIqqcNMuv8Znv3Tqx06z0P/fk+u/PjALeq7fv16pk+fznHHHce8efO45ZZbeOyxx8hMjjrqKI455hgOP/zwsv+dd97JmDFj+NrXvsaGDRtYt24dCxYsYNmyZTz9dG0AhpdffhmAcePGccMNN3DMMcdwxRVXMGnSpDJo1q9fz+OPP86DDz7IpEmTePjhhzt0HdSTWwqSdnivv/46Q4cOpaGhgX79+nH++efzy1/+klNOOYUePXrQs2dPTj311M2+xQ8bNoxbbrmFiRMn8tRTT9GrVy8GDBjA888/z8UXX8xDDz3EnnvuySuvvMLLL7/MMcccA8C5557L7Nlvj+156qmnAnDkkUeyZMmSur3uKrilIKnDbOk3+o628ZhCex199NHMnj2bBx54gPHjx3PppZcybtw4nnjiCX76058yZcoUfvjDH5a7kFrSrVs3oHbAe/369Vv1GrYXbilI2imNGjWKe++9l3Xr1vGXv/yFadOmMWrUqE36/OEPf2Dffffl05/+NBdccAHz589n1apVvPXWW5x22ml8/etfZ/78+ey111707t273NK4/fbby62GnY1bCpJ2SkcccQTjx49n+PDaiP0XXHDBJscTAGbNmsU111xD165d6dmzJ7fddhvLli3jvPPO46233gLgG9/4BgBTp05lwoQJrFu3jgEDBnDLLbfU9wXVSdTGpNtx+E92pO3LwoULOeSQQzq7DDXS3HsSEfMys6Gt57r7SJJUMhQkSSVDQZJUMhQkSSVDQZJUMhQkSSVDQdIOb9q0aQwdOnST22677cb06dM7dDmzZs3ipJNOarPf5MmTOeSQQzjnnHO49dZb+dznPtfmfLdm5Nb+/fuzatWqdj+vNYaCpB3eKaecwoIFC8rbZz/7WUaNGsWYMWPafG5mlheqdZTvfve7zJw5kzvuuGOL+m9tKFTBUJC0U/nd737HVVddxe23385uu+3GNddcw7BhwxgyZAhXXnklAEuWLOHggw9m3LhxDBo0iBdeeIG77rqLwYMHM2jQIC677LI2lzNx4kQ+9alPceyxxzJgwAAmT54MwIQJE3j++ec5/vjjNxsz6f777+eoo47i8MMP52Mf+xgvvvgiS5YsYcqUKVx33XUMHTqUOXPmsHLlSk477TSGDRvGsGHD+NWvfgXA6tWrGT16NAMHDuSCCy6giouPHeZCUseZ/lX481MdO8+/GwzHf3OLur755pucffbZXHvttfTr148ZM2awaNEiHn/8cTKTsWPHMnv2bPr168eiRYuYOnUqI0aM4E9/+hOXXXYZ8+bNo3fv3owePZp7772XT3ziE60u77nnnuORRx7htdde4+CDD+aiiy5iypQpPPTQQzzyyCPss88+3HrrrWX/kSNH8uijjxIR3HTTTVx99dVce+21TJgwgZ49e/LlL38ZgLPPPpsvfvGLjBw5kj/+8Y+MGTOGhQsXMmnSJEaOHMkVV1zBAw88wM0337zVq7UlhoKknca//uu/MnDgQM4880wAZsyYwYwZM8oxj9auXcuiRYvo168f73nPexgxYgQAv/nNbzj22GPp06cPAOeccw6zZ89uMxROPPFEunXrRrdu3Xj3u9/Niy++SN++fVvsv3TpUs4880yWL1/O3/72Nw488MBm+z388MM8++yz5f1XX32VtWvXMnv2bO65555y2b17997CNbPlDAVJHWcLv9FXYdasWfz4xz9m/vz5ZVtmcvnll/OZz3xmk75LliyhR48ebc5z2rRpTJo0CYCbbrpps8c3DpkNWzZs9sUXX8yll17K2LFjmTVrFhMnTmy231tvvcWjjz5K9+7d26yxo3lMQdIOb82aNZx33nncdttt9OrVq2wfM2YM3//+91m7di0Ay5YtY8WKFZs9f/jw4fziF79g1apVbNiwgbvuuotjjjlmkwPYDQ1tjiXXpldeeYX9998fqI26ulGvXr147bXXyvujR4/mhhtuKO9v/F8RRx99NHfeeScA06dPL/8laEcyFCTt8KZMmcKKFSu46KKLNjktdc2aNZx99tl84AMfYPDgwZx++umbfPhutN9++/HNb36TD3/4wxx22GEceeSRnHzyyR1e58SJEznjjDM48sgj2Weffcr2j3/84+VptXPmzGHy5MnMnTuXIUOGcOihhzJlyhQArrzySmbPns3AgQO555576NevX4fXWPnQ2RHRBZgLLMvMk5o8Nh64BlhWNH0nMzffRmvEobOl7YtDZ29/tmXo7HocU7gEWAjs2cLjd2dm61d2SJLqotLdRxHRFzgRaPXbvyRp+1D1MYXrga8ArV0ueFpEPBkRP4qIAyquR1IFdrT/4Lgz29b3orJQiIiTgBWZOa+VbvcD/TNzCDATmNpcp4i4MCLmRsTclStXVlCtpK3VvXt3Vq9ebTBsBzKT1atXb9OprJUdaI6IbwCfBNYD3akdU7gnM/+phf5dgJcyc6/W5uuBZmn78uabb7J06VLeeOONzi5F1EK6b9++dO3adZP2Tj/QnJmXA5cXxRwLfLlpIETEfpm5vLg7ltoBaUk7kK5du7Z4Za52PHW/ojkirgLmZuZ9wOcjYiy1rYmXgPH1rkeS9LbKr1PoaO4+kqT229LdR17RLEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpJKhIEkqGQqSpFLloRARXSLiPyPiJ8081i0i7o6IxRHxWET0r7oeSVLL6rGlcAmwsIXHzgfWZOZ7geuAb9WhHklSCyoNhYjoC5wI3NRCl5OBqcX0j4CPRkRUWZMkqWVVbylcD3wFeKuFx/cHXgDIzPXAK8DeTTtFxIURMTci5q5cubKqWiVpl1dZKETEScCKzJy3rfPKzBszsyEzG/r06dMB1UmSmlPllsKHgLERsQT4AfCRiPiPJn2WAQcARMTuwF7A6gprkiS1orJQyMzLM7NvZvYHzgJ+npn/1KTbfcC5xfTpRZ+sqiZJUut2r/cCI+IqYG5m3gfcDNweEYuBl6iFhySpk9QlFDJzFjCrmL6iUfsbwBn1qEGS1DavaJYklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklQwFSVLJUJAklbY6FCLiCx1ZiCSp823LlsKlHVaFJGm7sC2hEB1WhSRpu7AtoZAdVoUkabvQaihExGsR8Wozt9eA/dt4bveIeDwinoiIZyJiUjN9xkfEyohYUNwu2MbXI0naBru39mBm9tqGef8V+Ehmro2IrsAvI2J6Zj7apN/dmfm5bViOJKmDbMvZR39s7fGsWVvc7Vrc3OUkSduxSg80R0SXiFgArABmZuZjzXQ7LSKejIgfRcQBLcznwoiYGxFzV65cuQ0lS5JaU+mB5szckJlDgb7A8IgY1KTL/UD/zBwCzASmtjCfGzOzITMb+vTpsw0lS5Ja0+oxhYho6VqEAHpu6UIy8+WIeAQ4Dni6UfvqRt1uAq7e0nlKkjpeW1sKvVq49QT+Z2tPjIg+EfHOYnoP4B+A55r02a/R3bHAwvYUL0nqWG2dfbTZaaTtsB8wNSK6UAufH2bmTyLiKmBuZt4HfD4ixgLrgZeA8duwPEnSNorMlg8NRMQVrTw3M/N/dHxJrWtoaMi5c+fWe7GStEOLiHmZ2dBWv1a3FIC/NNPWAzgf2BuoeyhIkqrT1u6jazdOR0Qv4BLgPOAHwLUtPU+StGNqa0uBiHgXtRFRz6F2yugRmbmm6sIkSfXX1imp1wCnAjcCgxtdoSxJ2gm1dUrql4C/B/478KfGA+JFxKvVlydJqqe2jin47zolaRfih74kqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqVRZKERE94h4PCKeiIhnImJSM326RcTdEbE4Ih6LiP5V1SNJaluVWwp/BT6SmYcBQ4HjImJEkz7nA2sy873AdcC3KqxHktSGykIha9YWd7sWt2zS7WRgajH9I+CjERFV1SRJal2lxxQioktELABWADMz87EmXfYHXgDIzPXAK8DezcznwoiYGxFzV65cWWXJkrRLqzQUMnNDZg4F+gLDI2LQVs7nxsxsyMyGPn36dGyRkqRSXc4+ysyXgUeA45o8tAw4ACAidgf2AlbXoyZJ0uaqPPuoT0S8s5jeA/gH4Lkm3e4Dzi2mTwd+nplNjztIkupk9wrnvR8wNSK6UAufH2bmTyLiKmBuZt4H3AzcHhGLgZeAsyqsR5LUhspCITOfBA5vpv2KRtNvAGdUVYMkqX28olmSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVDIUJEklQ0GSVKosFCLigIh4JCKejYhnIuKSZvocGxGvRMSC4nZFVfVIktq2e4XzXg98KTPnR0QvYF5EzMzMZ5v0m5OZJ1VYhyRpC1W2pZCZyzNzfjH9GrAQ2L+q5UmStl1djilERH/gcOCxZh7+QEQ8ERHTI2JgPeqRJDWvyt1HAERET+DHwBcy89UmD88H3pOZayPiBOBe4KBm5nEhcCFAv379Kq5YknZdlW4pRERXaoFwR2be0/TxzHw1M9cW0w8CXSNin2b63ZiZDZnZ0KdPnypLlqRdWpVnHwVwM7AwM7/dQp+/K/oREcOLelZXVZMkqXVV7j76EPBJ4KmIWFC0/QvQDyAzpwCnAxdFxHrgdeCszMwKa5IktaKyUMjMXwLRRp/vAN+pqgZJUvt4RbMkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKhoIkqWQoSJJKlYVCRBwQEY9ExLMR8UxEXNJMn4iIyRGxOCKejIgjqqpHktS23Suc93rgS5k5PyJ6AfMiYmZmPtuoz/HAQcXtKOB7xU9JUieobEshM5dn5vxi+jVgIbB/k24nA7dlzaPAOyNiv6pqkiS1rsothVJE9AcOBx5r8tD+wAuN7i8t2pY3ef6FwIXF3bUR8dutLGUfYNVWPrdK22tdsP3WZl3tY13tszPW9Z4t6VR5KERET+DHwBcy89WtmUdm3gjc2AG1zM3Mhm2dT0fbXuuC7bc262of62qfXbmuSs8+ioiu1ALhjsy8p5kuy4ADGt3vW7RJkjpBlWcfBXAzsDAzv91Ct/uAccVZSCOAVzJzeQt9JUkVq3L30YeATwJPRcSCou1fgH4AmTkFeBA4AVgMrAPOq7Ae6IBdUBXZXuuC7bc262of62qfXbauyMyqlyFJ2kF4RbMkqWQoSJJKu0woRMRxEfHbYkiNr9Z52c0O+REREyNiWUQsKG4nNHrO5UWtv42IMRXWtiQiniqWP7doe1dEzIyIRcXP3kV7XYYliYiDG62TBRHxakR8oTPWV0R8PyJWRMTTjdravX4i4tyi/6KIOLeiuq6JiOeKZU+LiHcW7f0j4vVG621Ko+ccWbz/i4vao4K62v2+dfTfawt13d2opiUbj33WeX219NnQeb9jmbnT34AuwO+BAcA7gCeAQ+u4/P2AI4rpXsDvgEOBicCXm+l/aFFjN+DAovYuFdW2BNinSdvVwFeL6a8C3yqmTwCmAwGMAB6r03v3Z2oX3tR9fQFHA0cAT2/t+gHeBTxf/OxdTPeuoK7RwO7F9Lca1dW/cb8m83m8qDWK2o+voK52vW9V/L02V1eTx68FruiE9dXSZ0On/Y7tKlsKw4HFmfl8Zv4N+AG1ITbqIrdsyI/GTgZ+kJl/zcz/R+3srOHVV7rJ8qcW01OBTzRqr/ewJB8Ffp+Zf2ilT2XrKzNnAy81s7z2rJ8xwMzMfCkz1wAzgeM6uq7MnJGZ64u7j1K77qdFRW17ZuajWftkua3Ra+mwulrR0vvW4X+vrdVVfNv/r8Bdrc2jovXV0mdDp/2O7Sqh0NJwGnUXmw/58bliM/D7GzcRqW+9CcyIiHlRG04EYN98+3qRPwP7dkJdG53Fpn+snb2+oP3rpzPW26eofaPc6MCI+M+I+EVEjCra9i9qqUdd7Xnf6r2+RgEvZuaiRm11X19NPhs67XdsVwmF7UJsPuTH94D/AgylNt7TtZ1Q1sjMPILaiLX/HBFHN36w+EbUKectR8Q7gLHA/y6atof1tYnOXD8tiYivURul+I6iaTnQLzMPBy4F7oyIPetY0nb3vjXxj2z6xaPu66uZz4ZSvX/HdpVQ6PThNKKZIT8y88XM3JCZbwH/ztu7POpWb2YuK36uAKYVNby4cbdQ8XNFvesqHA/Mz8wXixo7fX0V2rt+6lZfRIwHTgLOKT5MKHbPrC6m51HbX/++oobGu5gqqWsr3rd6rq/dgVOBuxvVW9f11dxnA534O7arhMJvgIMi4sDi2+dZ1IbYqItin+VmQ3402R9/CrDxzIj7gLMioltEHEjt/008XkFdPaL2vy6IiB7UDlQ+XSx/49kL5wL/p1Fd9RyWZJNvcJ29vhpp7/r5KTA6InoXu05GF20dKiKOA74CjM3MdY3a+0REl2J6ALX183xR26sRMaL4HR3X6LV0ZF3tfd/q+ff6MeC5zCx3C9VzfbX02UBn/o5ty5HzHelG7aj976il/tfqvOyR1Db/ngQWFLcTgNuBp4r2+4D9Gj3na0Wtv2Ubz3Bopa4B1M7seAJ4ZuN6AfYGfgYsAh4G3lW0B/BvRV1PAQ0VrrMewGpgr0ZtdV9f1EJpOfAmtf2052/N+qG2j39xcTuvoroWU9uvvPF3bErR97Ti/V0AzAc+3mg+DdQ+pH8PfIdilIMOrqvd71tH/702V1fRfiswoUnfeq6vlj4bOu13zGEuJEmlXWX3kSRpCxgKkqSSoSBJKhkKkqSSoSBJKhkKUiEiNsSmo7N22Gi6URt58+m2e0qdq8p/xyntaF7PzKGdXYTUmdxSkNoQtbH2r47aOPqPR8R7i/b+EfHzYqC3n0VEv6J936j9P4MnitsHi1l1iYh/j9q4+TMiYo+i/+ejNp7+kxHxg056mRJgKEiN7dFk99GZjR57JTMHU7uK9fqi7QZgamYOoTb43OSifTLwi8w8jNoY/s8U7QcB/5aZA4GXqV05C7Xx8g8v5jOhqhcnbQmvaJYKEbE2M3s2074E+EhmPl8MXvbnzNw7IlZRG7LhzaJ9eWbuExErgb6Z+ddG8+hPbbz7g4r7lwFdM/PrEfEQsBa4F7g3M9dW/FKlFrmlIG2ZbGG6Pf7aaHoDbx/TO5HaeDZHAL8pRu6UOoWhIG2ZMxv9/L/F9K+pjeAJcA4wp5j+GXARQER0iYi9WpppROwGHJCZjwCXAXsBm22tSPXiNxLpbXtE8c/bCw9l5sbTUntHxJPUvu3/Y9F2MXBLRPw3YCVwXtF+CXBjRJxPbYvgImojdDanC/AfRXAEMDkzX+6wVyS1k8cUpDYUxxQaMnNVZ9ciVc3dR5KkklsKkqSSWwqSpJKhIEkqGQqSpJKhIEkqGQqSpNL/B+TUNNnCvlOYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(hist_p.history['loss'],linestyle='-.')\n",
    "plt.plot(hist_p.history['val_loss'])\n",
    "#plt.plot(hist_zi.history['loss'])\n",
    "plt.plot(hist_zi.history['val_loss'])\n",
    "plt.ylim(2,5)\n",
    "#plt.legend(['Poisson loss','Poisson val_loss','ZI loss','ZI val_loss'])\n",
    "plt.legend(['Poisson','Zero-Inflated'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.savefig('ch05_flik.pdf')\n",
    "np.mean(hist_p.history['val_loss'][1000:]) ,np.mean(hist_zi.history['val_loss'][1000:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3811529, 0.7653246)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_zi = tf.keras.models.Model(inputs=inputs, outputs=p_y_zi.mean())\n",
    "y_hat_te = model_zi(X_te)\n",
    "#mean_absolute_error(y_hat_te, y_te), mean_squared_error(y_hat_te, y_te)\n",
    "np.sqrt(np.mean((y_hat_te - y_te)**2)),np.mean(np.abs(y_hat_te - y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set 0.41930804 37\n",
      "Validation Set 0.26694542 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-1,5)\n",
    "plt.ylim(-1,5)\n",
    "y_hat_tr = model_zi(X_tr)\n",
    "\n",
    "plt.scatter(y_hat_te, np.random.normal(loc=y_te,scale=0.2), alpha=0.7)\n",
    "plt.xlabel('Counts: predicted by ZI (Training)')\n",
    "plt.ylabel('Counts: true')\n",
    "\n",
    "plt.savefig('ch05_zi.pdf')\n",
    "print('Training Set', np.mean(np.square(y_hat_tr[y_tr == 0])),np.sum(y_hat_tr[y_tr == 0]>0.5))\n",
    "print('Validation Set', np.mean(np.square(y_hat_te[y_te == 0])),np.sum(y_hat_te[y_te == 0]>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
