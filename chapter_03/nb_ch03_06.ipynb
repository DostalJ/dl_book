{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2sP8Kwe7L9Z"
   },
   "source": [
    "\n",
    "## Fitting a linear regression model with TF Eager\n",
    "\n",
    "**Goal:** In this notebook you will see how to use TF Eager to fit the parameters (slope and intercept) of a simple linear regression model via gradient descent (GD). \n",
    "\n",
    "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it. \n",
    "\n",
    "**Dataset:** You work again with the systolic blood pressure and age data of 33 American women, which is generated and visualized in the upper part of the notebook.\n",
    "\n",
    "**Content:**\n",
    "\n",
    "* fit a linear model via the sklearn machine learning library of python to get the fitted values of the intercept and slope as reference. \n",
    "\n",
    "* use the tensorflow library and enable the tf eager mode to fit the parameters of the simple linear model via GD with the objective to minimize the MSE loss. \n",
    "    * define the mse loss function \n",
    "    * apply the gradients_function of tf eager on the loss w.r.t. the parameters\n",
    "    * use the gradients to update the parameter values via update formula\n",
    "    * iterate over the two former steps for many steps and check the current values of the estimated model parameters and the loss after each updatestep \n",
    "    * verify that the estimated parameter values converge to the values which you got from the sklearn fit.  \n",
    "\n",
    "\n",
    "[open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_03/nb_ch03_06.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0-rc0  running in colab?:  False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try: #If running in colab \n",
    "    import google.colab\n",
    "    IN_COLAB = True \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if (not tf.__version__.startswith('2')): #Checking if tf 2.0 is installed\n",
    "    if IN_COLAB: #If running in colab install tf 2.0\n",
    "        !pip install tensorflow==2.0.0 \n",
    "    print('Please install tensorflow 2.0 to run this notebook')\n",
    "print('Tensorflow version: ',tf.__version__, ' running in colab?: ', IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMlU4PJJ5QO7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3c9bh7zMVhP"
   },
   "source": [
    "Here we read in the systolic blood pressure and the age of the 33 American women in our dataset. Then we use the sklearn library to find the optimal values for the slope a and the intercept b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvINwW1vydo9"
   },
   "outputs": [],
   "source": [
    "# Blood Pressure data\n",
    "x = [22, 41, 52, 23, 41, 54, 24, 46, 56, 27, 47, 57, 28, 48, 58,  9, \n",
    "     49, 59, 30, 49, 63, 32, 50, 67, 33, 51, 71, 35, 51, 77, 40, 51, 81]\n",
    "y = [131, 139, 128, 128, 171, 105, 116, 137, 145, 106, 111, 141, 114, \n",
    "     115, 153, 123, 133, 157, 117, 128, 155, 122, 183,\n",
    "     176,  99, 130, 172, 121, 133, 178, 147, 144, 217] \n",
    "x = np.asarray(x, np.float32) \n",
    "y = np.asarray(y, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "colab_type": "code",
    "id": "OurbybEoydpB",
    "outputId": "2887d477-074a-47d1-9e50-65a5b483b997"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept =  87.67143 solpe =  1.1050216\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(x=x,y=y)\n",
    "plt.title(\"blood pressure vs age\")\n",
    "plt.xlabel(\"x (age)\")\n",
    "plt.ylabel(\"y (sbp)\")\n",
    "\n",
    "model = LinearRegression()\n",
    "res = model.fit(x.reshape((len(x),1)), y)\n",
    "predictions = model.predict(x.reshape((len(x),1)))\n",
    "plt.plot(x, predictions)\n",
    "plt.show()\n",
    "print(\"intercept = \",res.intercept_,\"solpe = \", res.coef_[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxlyNp9uSKYd"
   },
   "source": [
    "## TF Eager\n",
    "\n",
    "Now we want to use the TF Eager mode to find the optimal parameters for the linear regression problem. First we define our mse loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQI88BUK_FN0"
   },
   "outputs": [],
   "source": [
    "def loss(a, b):\n",
    "  y_hat = a*x + b\n",
    "  return tf.reduce_mean((y_hat - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzQzM4Kq56Ai"
   },
   "source": [
    "We can do a forwad pass and can calculate the loss (mse) with the initial values a=0 and b=139 (139 is the mean of the blood pressure and slope a=0 implies that the model predicts the mean for each age). Note that `a` and `b` must be `tf.Variables`. We see the loss value in the numpy argument in the output. In contrast to old (pre 2.0) TensorFlow we don't need to run a session to get loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "nYbreWHHcX0e",
    "outputId": "62d2fac4-a285-4f81-a742-2a48c9d71b25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25, shape=(), dtype=float32, numpy=673.4545>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable(0.0)\n",
    "b = tf.Variable(139.0)\n",
    "loss(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4gDPW676omn"
   },
   "source": [
    "Now we define that we want to have the gradients of the loss w.r.t to our two model parameters, the slope `a` and the intercept `b`. In the next cell we print the gradient of the loss w.r.t `a` and gradient of the loss w.r.t to `b`. Note that we calculated the loss for all data points and therefore we get different gradients compared to nb_04, where we only used one datapoint. To be able to calculate the gradient of the loss, we must store the itermediate values needed for calculating the gradients in an object called tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "VC0mP77EFwWX",
    "outputId": "83e7d2f7-383b-4526-ff58-2385e8ec6b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at  tf.Tensor(673.4545, shape=(), dtype=float32)\n",
      "tf.Tensor(-553.09094, shape=(), dtype=float32) tf.Tensor(0.727273, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(0.0)\n",
    "b = tf.Variable(139.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  loss_val = loss(a,b)\n",
    "  print(\"Loss at \", loss_val)\n",
    "  grad_a, grad_b = tape.gradient(loss_val, [a,b])\n",
    "  print(grad_a, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A3u3ksK39s70"
   },
   "source": [
    "Now, let's use gradient descent to opimize the slope a and the intercept b. The start values are a=0 and b=139, our learning rate eta is 0.0004 and we do 80000 updatesteps with all 33 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "HCasZHdLWmG9",
    "outputId": "ce82424d-0904-45c8-bdb0-850f06d8f8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 slope= 0.22123638 intercept= 138.99971 gradient_a -553.09094 gradient_b 0.727273 mse= 673.4545 time for 1000 epochs  0.0006892204284667969\n",
      "Epoch: 5000 slope= 0.47009143 intercept= 120.60784 gradient_a -0.14053345 gradient_b 7.3059855 mse= 469.57272 time for 1000 epochs  2.417998027801514\n",
      "Epoch: 10000 slope= 0.6975813 intercept= 108.807014 gradient_a -0.09031677 gradient_b 4.688313 mse= 398.76877 time for 1000 epochs  2.401115131378174\n",
      "Epoch: 15000 slope= 0.8435634 intercept= 101.23433 gradient_a -0.058013916 gradient_b 3.0085347 mse= 369.61255 time for 1000 epochs  2.465251398086548\n",
      "Epoch: 20000 slope= 0.9372412 intercept= 96.37488 gradient_a -0.036483765 gradient_b 1.9306204 mse= 357.6062 time for 1000 epochs  3.0992573261260987\n",
      "Epoch: 25000 slope= 0.99735403 intercept= 93.256584 gradient_a -0.023651123 gradient_b 1.2389097 mse= 352.66214 time for 1000 epochs  2.6289466381073\n",
      "Epoch: 30000 slope= 1.0359306 intercept= 91.25547 gradient_a -0.016540527 gradient_b 0.7949929 mse= 350.62622 time for 1000 epochs  2.828418016433716\n",
      "Epoch: 35000 slope= 1.0606831 intercept= 89.97145 gradient_a -0.010864258 gradient_b 0.5101743 mse= 349.78793 time for 1000 epochs  2.7590006828308105\n",
      "Epoch: 40000 slope= 1.0765644 intercept= 89.14761 gradient_a -0.0056152344 gradient_b 0.32746363 mse= 349.4427 time for 1000 epochs  2.9034304141998293\n",
      "Epoch: 45000 slope= 1.0867523 intercept= 88.61913 gradient_a -0.004211426 gradient_b 0.21021843 mse= 349.30048 time for 1000 epochs  2.828148078918457\n",
      "Epoch: 50000 slope= 1.0932869 intercept= 88.28015 gradient_a -0.0022277832 gradient_b 0.13503599 mse= 349.242 time for 1000 epochs  2.7961384296417235\n",
      "Epoch: 55000 slope= 1.0975146 intercept= 88.060844 gradient_a -0.0016784668 gradient_b 0.08638334 mse= 349.21765 time for 1000 epochs  2.795926332473755\n",
      "Epoch: 60000 slope= 1.1001551 intercept= 87.923874 gradient_a -0.0010681152 gradient_b 0.05599904 mse= 349.20798 time for 1000 epochs  2.8734206199645995\n",
      "Epoch: 65000 slope= 1.1018678 intercept= 87.83504 gradient_a -0.0016021729 gradient_b 0.03627515 mse= 349.20377 time for 1000 epochs  2.5472561836242678\n",
      "Epoch: 70000 slope= 1.1029379 intercept= 87.77952 gradient_a -0.0006713867 gradient_b 0.023973227 mse= 349.20218 time for 1000 epochs  2.6364916801452636\n",
      "Epoch: 75000 slope= 1.1036732 intercept= 87.74137 gradient_a 0.00010681152 gradient_b 0.015524149 mse= 349.20145 time for 1000 epochs  2.496338939666748\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "a  = tf.Variable(0.) \n",
    "b = tf.Variable(139.0) \n",
    "eta = 0.0004  \n",
    "start= time()\n",
    "for i in range(80000): \n",
    "  with tf.GradientTape() as tape: \n",
    "    y_hat = a*x + b  \n",
    "    loss = tf.reduce_mean((y_hat - y)**2) \n",
    "    grad_a, grad_b  = tape.gradient(loss, [a,b]) \n",
    "    a = tf.Variable(a - eta * grad_a) \n",
    "    b = tf.Variable(b - eta * grad_b) \n",
    "    if (i % 5000 == 0): \n",
    "        t = time() - start\n",
    "        print(\"Epoch:\",i, \"slope=\",a.numpy(),\"intercept=\",b.numpy(),\"gradient_a\", grad_a.numpy(), \"gradient_b\",grad_b.numpy(), \"mse=\", loss.numpy(), \"time for 1000 epochs \", t/5.)\n",
    "        start = time()\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kt9XXxcH8d3k"
   },
   "source": [
    "Let's look at the final values for the slope a,the intercept b and the mse loss. We know form the closed formula solution that:\n",
    "\n",
    "1.   optimal value for a: 1.1050216\n",
    "2.   optimal value for b: 87.67143\n",
    "3.   minimal loss: 349.200787168560\n",
    "\n",
    "After 80000 update steps we are very close to the optimal values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "y_46N75AEbuM",
    "outputId": "48c140f0-1704-402e-ed20-9ecd203924c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1041987 87.71413 349.20105\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy(), b.numpy(), loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making it faster with tf.function\n",
    "\n",
    "Eager is really great when you start building a network from scratch. You can use debugging and immediately spot problems as they occur. You don't have to compile a graph first and then run it. \n",
    "\n",
    "However, if you use many small operations this might become slow. A way to speed this up, is the use of the `@tf.function` decorator. See https://www.tensorflow.org/guide/function for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 slope= 0.22123638 intercept= 138.99971 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.024369335174560545\n",
      "Epoch: 5000 slope= 0.47009143 intercept= 120.60784 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.26144886016845703\n",
      "Epoch: 10000 slope= 0.6975813 intercept= 108.807014 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.25389609336853025\n",
      "Epoch: 15000 slope= 0.8435634 intercept= 101.23433 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.25403523445129395\n",
      "Epoch: 20000 slope= 0.9372412 intercept= 96.37488 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2569833755493164\n",
      "Epoch: 25000 slope= 0.99735403 intercept= 93.256584 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2565422058105469\n",
      "Epoch: 30000 slope= 1.0359306 intercept= 91.25547 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2558902263641357\n",
      "Epoch: 35000 slope= 1.0606831 intercept= 89.97145 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2539824962615967\n",
      "Epoch: 40000 slope= 1.0765644 intercept= 89.14761 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2690378189086914\n",
      "Epoch: 45000 slope= 1.0867523 intercept= 88.61913 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.26211256980895997\n",
      "Epoch: 50000 slope= 1.0932869 intercept= 88.28015 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2687190532684326\n",
      "Epoch: 55000 slope= 1.0975146 intercept= 88.060844 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2695228099822998\n",
      "Epoch: 60000 slope= 1.1001551 intercept= 87.923874 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.2636560440063477\n",
      "Epoch: 65000 slope= 1.1018678 intercept= 87.83504 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.26027679443359375\n",
      "Epoch: 70000 slope= 1.1029379 intercept= 87.77952 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.24962306022644043\n",
      "Epoch: 75000 slope= 1.1036732 intercept= 87.74137 gradient_a -0.00093078613 gradient_b 0.009457111 mse= 349.20105 time for 1000 epochs  0.26503739356994627\n"
     ]
    }
   ],
   "source": [
    "start= time()\n",
    "a  = tf.Variable(0.0)\n",
    "b = tf.Variable(139.0)\n",
    "eta = 0.0004\n",
    "\n",
    "@tf.function #Will tell tf to build a graph from this code\n",
    "def train_step():\n",
    "    y_hat = a*x + b\n",
    "    loss = tf.reduce_mean((y_hat - y)**2)   \n",
    "    grad_a, grad_b  = tape.gradient(loss, [a,b])\n",
    "    a.assign(a - eta * grad_a)\n",
    "    b.assign(b - eta * grad_b)\n",
    "\n",
    "for i in range(80000):\n",
    "  with tf.GradientTape() as tape: #Record the gradients from now on\n",
    "    train_step()\n",
    "    if (i % 5000 == 0):\n",
    "        t = time() - start\n",
    "        print(\"Epoch:\",i, \"slope=\",a.numpy(),\"intercept=\",b.numpy(),\"gradient_a\", grad_a.numpy(), \"gradient_b\",grad_b.numpy(), \"mse=\", loss.numpy(), \"time for 1000 epochs \", t/5.)\n",
    "        start = time()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nb_06.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
